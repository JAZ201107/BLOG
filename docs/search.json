[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a master student of Singapore Management University majoring in IT in Business with AI track. I am passionate about AI. I have a strong background in computer science and big data. I am proficient in Python. I have experience in machine learning, deep learning. I am a quick learner and a good team player. I like to learn new things summary and show them with each other. Currently I’m looking for a full-time job in AI field."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nSingapore Management University | Singapre Master of IT in Business(AI track) | Sept 2023 -\nUniversity of Wollongong | Computer Science B.S in Big Data | March 2020 - March 2023"
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nMachine Learning & Deep Learning\nComputer Vision\nNatural Language Processing\nReinforcement Learning\nMeta Learning\nGenerative Model\nUnsupervised Learning\nLarge Language Model\nConvex Optmization\nProbabilitic Graphical Model\nDeep Graph Learning"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About Me",
    "section": "Projects",
    "text": "Projects"
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html",
    "href": "posts/Overviews/Overview_PGM.html",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "",
    "text": "Probabilistic Graphical Models(PGMs) are a crucial tool for reasoning under uncertainty, merging probability theory with graph theory to represent and analyze complex relationships between variables. PGMs help address problems that require modeling uncertainty in areas like machine learning, artificial iintelligence and so on. They also provide a structured framework for deciion-makeing, making them highly relevant to fields such as deep learning and reinforcement learning.\nFor example, PGMs play a foundational role by inspiring architectures that manage uncertainty, such as Bayesian Neural Networks and vairation inference techniques. They allow neural networks to quantify uncertainty, enhancing robustness in fields like medical diagnosis and autonomous driving. In reinforcement leaning, PGM facilitate modeling environments with partially observable Markov Decision processes, where agents make decisions under uncertainty. PGMs help RL algorithms in state eatimation and policy learning by providing a framework to manage incomplelte information about the environment.\nIn this blog, we will go through some essential aspects of PGMs, including representation, inference, and learning methods. We will discuss the two main types of PGMs, Bayesian Networks and Markov Networks, and Factor Graphs which unify both. Next we will explore inference algorithms from two asepect: exact inference methods like Variable Elimination and approximate inference methods like Monte Carlo methods and Variational Inference. Finally, we will discuss learning methods like Maximum Likelihood Estimation, Maximum A Posteriori Estimation, Expectation Maximization and structure learning when we do not have the structure of the model. In the end, we will discuss some applications of PGMs and how to combine with deep nerual networks to build more robust models."
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#bayesian-networks",
    "href": "posts/Overviews/Overview_PGM.html#bayesian-networks",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Bayesian Networks",
    "text": "Bayesian Networks\nBayesian Networks also known as Belief Networks or Directed Acyclic Graphs(DAGs), are a type of PGM that represent the conditional dependencies between random variables. The nodes in a BN represent random variables, and the edges represent the direct dependencies between them. The conditional dependencies are encoded using the conditional probability distribution of each node given its parents.\nLet us consider a simple example of a student model, where the grade of a student depends on the difficulty of the course, the student’s intelligence, and the SAT score. The Bayesian Network for this model is shown below:\n\n\n\n\n\n\n\n\nG\n\n\n\nDifficulty\n\nDifficulty\n\n\n\nGrade\n\nGrade\n\n\n\nDifficulty-&gt;Grade\n\n\n\n\n\nLetter\n\nLetter\n\n\n\nGrade-&gt;Letter\n\n\n\n\n\nIntelligence\n\nIntelligence\n\n\n\nIntelligence-&gt;Grade\n\n\n\n\n\nSAT\n\nSAT\n\n\n\nIntelligence-&gt;SAT\n\n\n\n\n\n\n\n\nFigure 1: Student Model\n\n\n\n\n\nIn the Bayesian Network, the joint distribution can be factorized into the product of the conditional probabilities of each node given its parents. This factorization allows for efficient inference and learning in the model. The above Bayesian Network represents the following conditional dependencies: \\[\nP(D, I, S, G, L) = P(D)P(I)P(S|I)P(G|D, I)P(L|G)\n\\]\nSo, the joint probability can be factorized into conditional probabilties of each vairable given its parents. \\[\nP(X_1, X_2, \\dots, X_n) = \\prod_{i=1}^{n} P(X_i | \\text{Pa}(X_i))\n\\] where \\(\\text{Pa}(X_i)\\) represents the parents of node \\(X_i\\) in the Bayesian Network.\nOn the other hand, we can view the graph as encoding a generative sampling process executed in a topological order, starting from the root nodes and samples the child nodes given their parents. For example, in the student model, we can sample the grade of the student by first sampling the difficulty, intelligence, and SAT score, and then the grade and letter.\nThere is probabilic distirbution associalte with each node in the Bayesian Network called Conditional Probability Distribution(CPD). The CPD of a node \\(X_i\\) is the probability distribution of \\(X_i\\) given its parents \\(\\text{Pa}(X_i)\\). For example, the CPD of the grade node in the student model is: \\[\nP(G=A | D, I) = \\begin{cases}\n0.3 & \\text{if } D = \\text{Easy}, I = \\text{Low} \\\\\n0.7 & \\text{if } D = \\text{Easy}, I = \\text{High} \\\\\n0.9 & \\text{if } D = \\text{Hard}, I = \\text{Low} \\\\\n0.1 & \\text{if } D = \\text{Hard}, I = \\text{High} \\\\\n\\end{cases}\n\\]\nWhen we are given a Bayesian Network, we want to perform inference to answer queries about the model. For example, how the difficulty of the course change when we know the grade of the student.\n\n\n\n\n\n\nInference in Bayesian Networks\n\n\n\n\n\nInference in Bayesian Networks is the process of answering queries about the model, such as computing the posterior distribution of a variable given evidence.\n\n\n\nThe representation of the Bayesian Network of the above inference problem is shown below:\n\n\n\n\n\n\n\n\nG\n\n\n\nDifficulty\n\nDifficulty\n\n\n\nGrade\n\nGrade\n\n\n\nDifficulty-&gt;Grade\n\n\n\n\n\nLetter\n\nLetter\n\n\n\nGrade-&gt;Letter\n\n\n\n\n\nIntelligence\n\nIntelligence\n\n\n\nIntelligence-&gt;Grade\n\n\n\n\n\nSAT\n\nSAT\n\n\n\nIntelligence-&gt;SAT\n\n\n\n\n\n\n\n\nFigure 2: Inference in Bayesian Networks The goal is to compute the posterior distribution of the difficulty given the grade.\n\n\n\n\n\nWe use shadowed nodes to represent the observed variables, and the goal is to compute the posterior distribution of the difficulty \\(P(D|G)\\) given the grade.\nThere are two main inference algorithms for Bayesian Networks: * Exact Inference: Variable Elimination, Belief Propagation * Approximate Inference: Gibbs Sampling, Variational Inference We leave the details of the inference in the Bayesian Networks for the next section.\nThe next question raise is the when we are given are dataset, how can we learn the structure and parameters of the Bayesian Network. This is where the learning methods come into play.\nNext Let introduce I-Maps. I-Maps are the independencies that are implied by the graph structure of the Bayesian Network. It establish the relationship between graph and distribution. A distribution The I-Maps are used to perform efficient inference in the model by exploiting the conditional independence relationships between the variables. For example, in the student model, the grade is independent of the SAT score given the intelligence of the student. This independence relationship can be used to simplify the computation of the posterior distribution of the grade given the SAT score and intelligence. Formally, a graph \\(G\\) is called an I-map of probability distribution \\(P\\) if all conditional indeoendence relationships implied by the graph \\(G\\) aslo hold in teh distribution \\(P\\). This means that: \\[\nI(G) \\subseteq I(P)\n\\]\nOn the definition we can see that one probability distribution can have multiple I-Maps. The I-Maps are used to perform efficient inference in the model by exploiting the conditional independence relationships between the variables. For example, in the student model, the grade is independent of the SAT score given the intelligence of the student. This independence relationship can be used to simplify the computation of the posterior distribution of the grade given the SAT score and intelligence.\nOne Important thing about I-map is that it can not miss any independencies that are present in the distribution. If the graph \\(G\\) is an I-map of the distribution \\(P\\), then the graph \\(G\\) is called a perfect map of the distribution \\(P\\). This means that the graph \\(G\\) captures all the independencies in the distribution \\(P\\) and can be used for efficient inference and learning in the model.\nWhen we have a Bayesian Network, we determine whether two variables in the network are conditionally independent through D-separation. D-separation is a graphical criterion that determines whether two variables are conditionally independent given a set of observed variables. The D-separation criterion is based on the concept of active trails in the graph, which are paths between two variables that are not blocked by any observed variables. If there is an active trail between two variables, they are not conditionally independent given the observed variables. If all trails between two variables are blocked by the observed variables, they are conditionally independent. The D-separation criterion is used to determine the conditional independence relationships in the Bayesian Network and establish the I-maps of the model.\nD-Separation is used as a tool to determin whether a given graph can serve an I-map for a given distribution.\n\nApplications of Bayesian Networks\nBayesian Networks are widely used in various applications, such as: * Medical Diagnosis: Modeling the dependencies between symptoms and diseases. * Anomaly Detection: Detecting unusual patterns in network traffic or financial transactions. * Robotics: Modeling the environment for robot localization and mapping. * Natural Language Processing: Modeling the dependencies between words in a sentence."
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#markov-networks",
    "href": "posts/Overviews/Overview_PGM.html#markov-networks",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Markov Networks",
    "text": "Markov Networks\nMarkov Networks also named as Undirted Graphical Models.\nMarkov Networks are undirected graphs that represent the joint distribution of random variables. The nodes in a Markov Network represent random variables, and the edges represent the interactions between them. Unlike Bayesian Networks, Markov Networks do not have directed edges, and the interactions are symmetric. The joint distribution of the random variables is factorized into potential functions that capture the interactions between the variables.\nUnlike parent-child relationships in Markov network, we cannot assign a conditional probabilituy table to each node and apply the chain rule to compute the joint distribution as we do in Bayesian Networks. Instead, we assign a factor to each maximal clique in the graph, which is a subset of nodes that are fully connected. The joint distribution is then factorized into the product of potential functions over the cliques in the graph: \\[\nP(X_1, X_2, \\dots, X_n) = \\frac{1}{Z} \\prod_{C \\in \\mathcal{C}} \\phi_C(X_C)\n\\] where \\(\\mathcal{C}\\) is the set of maximal cliques in the graph, \\(\\phi_C(X_C)\\) is the potential function over the clique \\(C\\), and \\(Z\\) is the normalization constant called the partition function, defined as: \\[\nZ = \\sum_{X_1, X_2, \\dots, X_n} \\prod_{C \\in \\mathcal{C}} \\phi_C(X_C)\n\\]\nFor example:\n\nThe Markov Field is defiend as the Gibbs Distribution(Blozmann Distribution) over the graph. The Gibbs Distribution is defined as: \\[\nP(X) = \\frac{1}{Z} \\exp\\left(-\\sum_{C \\in \\mathcal{C}} \\phi_C(X_C)\\right)\n\\] where \\(Z\\) is the partition function, and \\(\\phi_C(X_C)\\) is the potential function over the clique \\(C\\).\nThe Hammersley-Clifford Theorem states that any positive distribution over a set of random variables can be represented as a Markov Network. This theorem provides a theoretical foundation for the use of Markov Networks to model complex dependencies between random variables.\nTo improve he Markove Network, notes that the distribution can not be nagetive, so we can not use the general Gibbs Distributions, because the potential function can be nagetive. To solve this problem, we can use the Exponential Form of the Gibbs distributions, which is always non-negative. We represent a clique potential \\(\\psi(x_c)\\) in an unconstrained form value “energy” function \\(\\psi(x_c)=\\exp(-\\phi_c(x_c))\\). The joint probability has a nice additive form: \\[\nP(X) = \\frac{1}{Z} \\exp\\left(-\\sum_{C \\in \\mathcal{C}} \\phi_C(X_C)\\right) = \\frac{1}{Z} \\exp\\left(-H(x)\\right)\n\\] where \\(H(x)\\) is the free energy function of the Markov Network. This is called the Bolztmann Distribution(also called log-linear model in statistics)."
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#factor-graph",
    "href": "posts/Overviews/Overview_PGM.html#factor-graph",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Factor Graph",
    "text": "Factor Graph\nFactor Graphs are a generalization of Bayesian Networks and Markov Networks that unify both representations. Factor Graphs represent the joint distribution of random variables using factors that capture the dependencies between the variables. The nodes in a Factor Graph represent random variables, and the edges represent the factors that connect the variables. Factor Graphs provide a flexible framework for modeling complex dependencies between random variables and can be used to represent both Bayesian Networks and Markov Networks. For example, the student model can be represented as a Factor Graph as shown below:\n\n\n\n\n\n\n\n\nG\n\n\n\nDifficulty\n\nDifficulty\n\n\n\nGrade\n\nGrade\n\n\n\nDifficulty--Grade\n\n\n\n\nLetter\n\nLetter\n\n\n\nGrade--Letter\n\n\n\n\nIntelligence\n\nIntelligence\n\n\n\nIntelligence--Grade\n\n\n\n\nSAT\n\nSAT\n\n\n\nIntelligence--SAT\n\n\n\n\n\n\n\nFigure 3: Factor Graph of Student Model\n\n\n\n\n\nIn the Factor Graph, the nodes represent random variables, and the edges represent the factors that connect the variables. The joint distribution of the random variables is factorized into the product of factors over the cliques in the graph: \\[\nP(X_1, X_2, \\dots, X_n) = \\frac{1}{Z} \\prod_{f \\in \\mathcal{F}} \\phi_f(X_f)\n\\] where \\(\\mathcal{F}\\) is the set of factors in the graph, \\(\\phi_f(X_f)\\) is the factor over the variables \\(X_f\\), and \\(Z\\) is the normalization constant called the partition function.\nFactor Graphs provide a unified representation of Bayesian Networks and Markov Networks and can be used to model complex dependencies between random variables. They are widely used in various applications, such as: * Error-Correcting Codes: Modeling the dependencies between bits in a code. * Sensor Networks: Modeling the dependencies between sensors in a network. * Image Processing: Modeling the dependencies between pixels in an image. * Social Networks: Modeling the dependencies between users in a network. * Robotics: Modeling the dependencies between sensors and actuators in a robot. * Natural Language Processing: Modeling the dependencies between words in a sentence."
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#exact-inference",
    "href": "posts/Overviews/Overview_PGM.html#exact-inference",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Exact Inference",
    "text": "Exact Inference\n\nThe Elimination Algorithms\n\nVariable Elimination\n\n\nBucket Elimination\n\n\n\nMessage Passing Algorithms\n\nBelief Propagation\n\n\nSum Product Algorithm\n\n\n\nThe Junction Tree Algorithm"
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#approximate-inference",
    "href": "posts/Overviews/Overview_PGM.html#approximate-inference",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Approximate Inference",
    "text": "Approximate Inference\n\nMonte Carlo Methods\n\nMarkov Chain Monte Carlo (MCMC)\n\n\n\nVariational Algorithms"
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#parameter-learning",
    "href": "posts/Overviews/Overview_PGM.html#parameter-learning",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Parameter Learning",
    "text": "Parameter Learning\n\nMaximum Likelihood Estimation\n\\[\n\\theta = \\underset{\\theta}{\\operatorname{arg\\max}\\ } P(\\mathcal{D} | \\theta)\n\\]\nFor more details check out this blog Maximum Likelihood Learning. ### Maximum A Posteriori Estimation\n\\[\n\\theta = \\underset{\\theta}{\\operatorname{arg\\max}} \\overbrace{ P(\\mathcal{D} |\\theta) }^{ \\text{Likelihood} }\\underbrace{ P(\\theta) }_{ \\text{Prior Distribution} }\n\\]\n\n\nExpectation Maximization\nThis is a general framework for learning in probabilistic models with latent variables. The EM algorithm alternates between the E-step, where the posterior distribution of the latent variables is computed, and the M-step, where the model parameters are updated to maximize the expected log-likelihood. The EM algorithm is used in models like Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) to estimate the parameters of the model when the latent variables are not observed.\nFor More Details, check out this blogs Expectation Maximization ## Structure Learning ### Chow-Liu Algorithm"
  },
  {
    "objectID": "posts/Overviews/Overview_MetaLearning.html",
    "href": "posts/Overviews/Overview_MetaLearning.html",
    "title": "Overview: Deep Meta Learning",
    "section": "",
    "text": "Overview of the Deep Meta Learning"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html",
    "href": "posts/Overviews/Overview_Optimization.html",
    "title": "Overview: Optimization Algorithms",
    "section": "",
    "text": "Optimization is one of the core components of machine learning and deep learning. The most part of the machine learning algorithm is to optimize the parameters in the objective function from given dataset \\(\\mathcal{D}\\). In the era of the immense data, the effectiveness and efficiency of the numerical optimization algorithms dramatically influence the popularization and application of the machine learning.\nMost of the optimization algorithms involve update the parameters according to the gradient information. From the perspective of the gradient information, popular optimization methods can be divided into three categories:\nMkae decision change what is the\nFirst Order Optimization methods are the most common methods used in the deep learning field, because it is easier to implement, while higher order methods converge at a faster speed with the curvature information makes the search direction more effective, is more computing demanding due to the operation and storage of the inverse matrix of the Hessian matrix. To solve this problem, many variants based on Newton’s method has been developed most of which try to approximate the Hessian matrix through some techniques.\nDerivative free optimization methods are mainly used in the case that the derivative of the objective function may not exits or be difficult to calculate, the two main ideas are that:"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#optimization-in-machine-learning",
    "href": "posts/Overviews/Overview_Optimization.html#optimization-in-machine-learning",
    "title": "Overview: Optimization Algorithms",
    "section": "Optimization in Machine Learning",
    "text": "Optimization in Machine Learning"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#optimization-in-deep-learning",
    "href": "posts/Overviews/Overview_Optimization.html#optimization-in-deep-learning",
    "title": "Overview: Optimization Algorithms",
    "section": "Optimization in Deep Learning",
    "text": "Optimization in Deep Learning"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#optimization-in-reinforcement-learning",
    "href": "posts/Overviews/Overview_Optimization.html#optimization-in-reinforcement-learning",
    "title": "Overview: Optimization Algorithms",
    "section": "Optimization in Reinforcement Learning",
    "text": "Optimization in Reinforcement Learning"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#optimization-in-meta-learning",
    "href": "posts/Overviews/Overview_Optimization.html#optimization-in-meta-learning",
    "title": "Overview: Optimization Algorithms",
    "section": "Optimization in Meta Learning",
    "text": "Optimization in Meta Learning"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#optimization-in-variation-inference",
    "href": "posts/Overviews/Overview_Optimization.html#optimization-in-variation-inference",
    "title": "Overview: Optimization Algorithms",
    "section": "Optimization in Variation Inference",
    "text": "Optimization in Variation Inference"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#optimization-in-generative-model",
    "href": "posts/Overviews/Overview_Optimization.html#optimization-in-generative-model",
    "title": "Overview: Optimization Algorithms",
    "section": "Optimization in Generative Model",
    "text": "Optimization in Generative Model"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#optimization-in-markov-chain-monte-carlo",
    "href": "posts/Overviews/Overview_Optimization.html#optimization-in-markov-chain-monte-carlo",
    "title": "Overview: Optimization Algorithms",
    "section": "Optimization in Markov Chain Monte Carlo",
    "text": "Optimization in Markov Chain Monte Carlo\nThis is the list\n\nList\nnext"
  },
  {
    "objectID": "posts/Readings/DLFC/Chapter02.html",
    "href": "posts/Readings/DLFC/Chapter02.html",
    "title": "Deep Learning Foundations and Concepts: Chapter 02 Summary and Exercise Solutions",
    "section": "",
    "text": "Deep Learning Foundations and Concepts Chapter 02 Summary and Exercise Solutions"
  },
  {
    "objectID": "posts/Basic/MDP.html",
    "href": "posts/Basic/MDP.html",
    "title": "From Markov Chain to Markov Decision Process and MCMC",
    "section": "",
    "text": "Markov Chain\n\n\nMarkov Decision Process\n\n\nMarkov Chain Monte Carlo"
  },
  {
    "objectID": "posts/Basic/EM.html",
    "href": "posts/Basic/EM.html",
    "title": "Expectation Maximization",
    "section": "",
    "text": "From K-Means Algorithms to EM\n\n\nFrom Gaussian Mixture Model\n\n\nGeneral Expectation Maximization Algorithms\n\n\nEvidence Lower Bound\n\n\nCombined With Deep Neural Network\n\n\nConclusion"
  },
  {
    "objectID": "posts/Papers/Attention_is_all_you_need.html",
    "href": "posts/Papers/Attention_is_all_you_need.html",
    "title": "Dive into: Attention is all you need",
    "section": "",
    "text": "This is article, I am going to implement the Transformer model from the paper Attention is all you need(Vaswani et al. (2023) ) This paper is the corner stone of the current state of the art in NLP and LLM. The Transformer model is the first model that uses only attention mechanism to model the relationship between words in a sentence. This model is the base of the current state of the art models like , GPT-2, etc.(Most code is referenced from (Klein et al. (2017)) )"
  },
  {
    "objectID": "posts/Basic/MLE.html",
    "href": "posts/Basic/MLE.html",
    "title": "Maximum Likelihood Learning: From Basic to Advance",
    "section": "",
    "text": "Maximum Likelihood Estimation is foundational across machine learning, helping estimation parameters by maximizing the likelihood of observed data. It serves as the backbone for various algorithms, from classical linear regression to deep neural networks. This blog walks through MLE’s foundations, how it apply to the modern deep learning era and how to solve the MLE problem when it lack a closed-form solution.\n\nThe Idea of Maximum Likelihood Estimation\nThe goal of MLE is to find the parameters of a probabilistic model that maximize the likelihood of the observed data. Formally, for a dataset \\(\\mathcal{D} = \\{x_1, x_2, \\dots, x_n \\}\\), we assume each data point \\(x_i\\) is independently generated from a probability distribution \\(p(x | \\theta)\\), parameterized by \\(\\theta\\). The MLE seeks the parameters \\(\\theta\\) that maximize the likelihood of the observed data, which is the product of the probability of each data point: \\[\nL(\\theta; \\mathcal{D}) = \\prod_{i=1}^{n} p(x_i | \\theta)\n\\]\nIn practice, the logarithm of the likelihood function, known as the log-likelihood, is maximized for numerical stability: \\[\n\\log L(\\theta; \\mathcal{D}) = \\sum_{i=1}^{n} \\log p(x_i | \\theta)\n\\] We can use optimization techniques like Stochastic Gradient Descent (SGD) to maximize the log-likelihood when a closed-form solution is not available. Specifically, MLE aims to maximize the likelihhod of the observed data given the model parameters, which translates into minimizing the negative log-likelihood(NNL) as the loss function. And optimize the loss function using numerical optimization techniques like SGD. For example\nIn the context of the multi-class classification problem, the likelihood for the entire dataset of zie \\(n\\) is given by: \\[\nL(\\theta; \\mathcal{D}) = \\prod_{i=1}^{n} p(y_i | x_i, \\theta)\n\\] where \\(y_i\\) is the label of the \\(i\\)-th data point \\(x_i\\). The log-likelihood is then: \\[\n\\log L(\\theta; \\mathcal{D}) = \\sum_{i=1}^{n} \\log p(y_i | x_i, \\theta)\n\\] The negative log-likelihood (NNL) is used as the loss function to be minimized: \\[\n\\text{NNL}(\\theta; \\mathcal{D}) = - \\sum_{i=1}^{n} \\log p(y_i | x_i, \\theta)\n\\] In the deep neural networks, the output layer for multi-class classfication is usually a softmax function. The softmax function converts raw output scores(logits) into probabilities for each class: \\[\np(y_i | x_i, \\theta) = \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\] where \\(f_j\\) is the raw output score for class \\(j\\), and \\(C\\) is the number of classes. The negative log-likelihood loss function for the softmax output is the cross-entropy loss: \\[\n\\text{NNL}(\\theta; \\mathcal{D}) = - \\sum_{i=1}^{n} \\log \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\] The cross-entropy loss for the entire dataset is the average of the loss for each data point: \\[\n\\mathcal{L}(\\theta; \\mathcal{D}) = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i, c}\\log \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\]\nWe can use SGD to minimize the cross-entropy loss by updating the model parameters \\(\\theta\\) in the opposite direction of the gradient of the loss function with respect to \\(\\theta\\).\n\n\nBasic MLE Example: Single Gaussian Density Estimation\nTo illustrate the concept of MLE, let’s consider a simple example of estimating the parameters of a single Gaussian distribution. Given a dataset \\(\\mathcal{D} = \\{x_1, x_2, \\dots, x_n \\}\\), the likelihood of the data under the Gaussian distribution is: \\[\nL(\\mu, \\sigma^2; \\mathcal{D}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\]\n\n\nIntermediate MLE Example: Linear Regression\nNext we go through a more complex example of MLE in the context of linear regression. Given a dataset \\(\\mathcal{D} = \\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) \\}\\), the linear regression model assumes the relationship between the input \\(x\\) and the output \\(y\\) is linear: \\[\ny = w x + b + \\epsilon\n\\] where \\(\\epsilon\\) is the noise term. The likelihood of the data under the linear regression model is:\nThe Likelihood of the data under the linear regression model is: \\[\nL(w, b; \\mathcal{D}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - w x_i - b)^2}{2\\sigma^2}\\right)\n\\] where \\(\\sigma^2\\) is the variance of the noise term \\(\\epsilon\\). The log-likelihood is: \\[\n\\log L(w, b; \\mathcal{D}) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - w x_i - b)^2\n\\]\nThe first term in the log-likelihood is a constant, so maximizing the log-likelihood is equivalent to minimizing the sum of squared errors between the predicted output \\(w x_i + b\\) and the true output \\(y_i\\). This is the same as the objective function in the ordinary least squares (OLS) method for linear regression. \\[\n\\text{OLS Loss} = \\sum_{i=1}^{n} (y_i - w x_i - b)^2\n\\]\n\n\nAdvanced MLE Example: Multi-Class Classification with Deep Learning\nFinally, we explore MLE in the context of multi-class classification using deep learning. Given a dataset \\(\\mathcal{D} = \\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) \\}\\), the deep neural network model predicts the probability distribution over the classes for each input \\(x_i\\): \\[\np(y_i | x_i, \\theta) = \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\] where \\(f_j\\) is the raw output score for class \\(j\\), and \\(C\\) is the number of classes. The negative log-likelihood loss function for the softmax output is the cross-entropy loss: \\[\n\\text{NNL}(\\theta; \\mathcal{D}) = - \\sum_{i=1}^{n} \\log \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\]\nWe can use SGD to minimize the cross-entropy loss by updating the model parameters \\(\\theta\\) in the opposite direction of the gradient of the loss function with respect to \\(\\theta\\).\n\n\nAdvanced MLE Exmples: MLE in Generative Model\nMLE is also used in generative models like Gaussian Mixture Models (GMM) and Variational Autoencoders (VAEs). In GMM, the likelihood of the data under the model is a mixture of Gaussian distributions. The MLE estimates the parameters of the Gaussian components to maximize the likelihood of the observed data. In VAEs, the MLE is used to train the encoder and decoder networks to maximize the likelihood of the data under the generative model.\nIn the generative model case, we want to learn a probability distribution \\(P(x)\\) over image to do: * Generation: If we sample \\(x_{\\text{new}} \\sim P(X)\\), \\(x_{\\text{new}}\\) should look like the real image (sampling) * Density Estimation: \\(P(x)\\) should be high if \\(x\\) looks like a real image (Anomaly Detecion) * Unsupervised Representation: We should be able to learn what these images have in common. But the question raise: 1. How to represent \\(P_{\\theta}(x)\\): the \\(x\\) is high-dimentional, Normal Distribution cannot be used here 2. How to learn the parameter of the \\(P_{\\theta}(x)\\)\nThe goal of learning is to return a model \\(P_{\\theta}(x)\\) that precisely captures the distributions \\(P_{\\text{data}}(x)\\) from which out data was sampled.\nHere we use KL-divergence to measure how two distribution are close to each other. \\[\n\\begin{split}\nD(p \\| q) & =  \\sum_{x}p(x)\\log \\frac{p(x)}{q(x)} \\\\\n& = \\mathbb{E}_{x \\sim p_{data}}\\left[ \\log \\left( \\frac{P_{\\text{data}}(x)}{P_{\\theta}(x)} \\right) \\right] \\\\\n& = \\mathbb{E}_{x \\sim p_{data}}[\\log P_{\\text{data}}(x)] - \\mathbb{E}_{x \\sim p_{data}}[\\log P_{\\theta}(x)]\n\\end{split}\n\\] Since the first term is the does not depend on \\(P_{\\theta}\\), so, we just need the maximize the second term to minimize the KL-divergence. That mean we need to maximizing the expected log-likelihood: \\[\n\\underset{P_{\\theta}}{\\operatorname{arg\\min}\\ } D(P_{data} \\| P_{\\theta}) = \\underset{P_{\\theta}}{\\operatorname{arg\\max}\\ } \\mathbb{E}_{x \\sim p_{data}}[\\log P_{\\theta}(x)]\n\\]\nSince we can not get the exact value of the \\(\\mathbb{E}_{x \\sim p_{data}}[\\log P_{\\theta}(x)]\\), we can use empirical log-likelihood: \\[\n\\mathbb{E}_{x \\sim p_{data}}[\\log P_{\\theta}(x)] \\approx \\frac{1}{|\\mathcal{D}|}\\sum_{x \\in \\mathcal{D}}\\log P_{\\theta}(x)\n\\]\n\n\nChallenges and Limitations of MLE\nFinally, we disscuss some of the challenges and limitations of MLE. One of the main challenges of MLE is that it requires a closed-form solution for the likelihood function, which is not always available, especially in complex models like deep neural networks. In such cases, numerical optimization techniques like SGD are used to maximize the log-likelihood. Another limitation of MLE is that it assumes the data is generated from a specific parametric model, which may not always be the case in practice. In such cases, more flexible models like non-parametric methods or Bayesian approaches may be more appropriate. There are some other limitations of MLE, such as overfitting, local optima, and sensitivity to outliers, which can be addressed by regularization techniques, initialization strategies, and robust loss functions.\n\n\nAlternative to MLE\nThere are some methods when the MLE is not good for the problem. Maximum Posterior Estimation (MAP) is one of the alternatives to MLE. MAP incorporates prior knowledge about the parameters into the estimation process by maximizing the posterior distribution of the parameters given the data. Bayesian methods are another alternative to MLE, which treat the parameters as random variables and estimate their posterior distribution using Bayes’ theorem. Bayesian methods provide a more principled way to incorporate prior knowledge and uncertainty into the estimation process.\n\n\nConclusion\nIn conclusion, Maximum Likelihood Estimation (MLE) is a fundamental concept in machine learning that aims to estimate the parameters of a probabilistic model by maximizing the likelihood of the observed data. MLE is used across a wide range of machine learning algorithms, from basic density estimation to advanced deep learning models. When a closed-form solution is not available, numerical optimization techniques like SGD can be used to maximize the log-likelihood. MLE has its challenges and limitations, such as the need for a closed-form solution and the assumption of a specific parametric model. However, MLE remains a powerful tool for parameter estimation in machine learning and provides a solid foundation for more advanced methods like Bayesian inference."
  },
  {
    "objectID": "posts/Readings/DLFC/Chapter03.html",
    "href": "posts/Readings/DLFC/Chapter03.html",
    "title": "Deep Learning Foundations and Concepts: Chapter 03 Summary and Exercise Solutions",
    "section": "",
    "text": "This is the content of what I learning in the DLFC.\nHighlights of thehioho\ndas a a da"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html",
    "href": "posts/Overviews/Overview_DeepRL.html",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "",
    "text": "In recent years, Deep Reinforcement Learning has emerged as one of the most promising areas within the broader field of Artificial Intelligence(AI). It combines two powerful tools: reinforcement learning(RL), where agents learn by interacting with their environment, and deep learning, where neural networks identify patterns from large amounts of data.\nThe Markov Decision Process provide the mathematical foundation for DRL, enabling agents to make intelligent decision in complex environment. Let defines some key concepts first, but for more details of the MDP, please check this blog"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#classical-deepq-learning",
    "href": "posts/Overviews/Overview_DeepRL.html#classical-deepq-learning",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Classical (Deep)Q-Learning",
    "text": "Classical (Deep)Q-Learning\nThe classic Q-learning finds the optimal state-action value function \\(Q^*(s, a)\\) through the Bellman Equation. The Q-value update rule is: \\[\nQ(s_{t}, a_{t}) \\leftarrow  Q(s_{t}, a_{t}) + \\alpha(R_{t + 1} + \\gamma \\underset{a'}{\\max \\ }Q(s_{t+1}, a') - Q(s_{t}, a_{t}))\n\\]\nWhen we use the neural network as function approximator, we need define an loss function to update the parameters. The Loss function is: \\[\n\\mathcal{L}(\\theta) = \\mathbb{E}[(R_{t+1} + \\gamma \\underset{a'}{\\max \\ }Q(s_{t+1}, a'; \\theta^{-}) - Q(s_{t}, a_{t}; \\theta) )^{2}]\n\\]\nThe problem of classical Q-Learning is that it suffer from overestimation bias, where the max operator selects actions that appear optimal due to noisy or high Q-value. To release this problem, we can use Double Q-Learning, it defined as"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#double-deep-q-learning",
    "href": "posts/Overviews/Overview_DeepRL.html#double-deep-q-learning",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Double (Deep) Q-Learning",
    "text": "Double (Deep) Q-Learning\nDouble Q-Learning(Hasselt, Guez, and Silver 2015) reduces overestimation bias by maintaining two Q-tables(or two networks in deep learning settings). Each table is updated independently to decouple the action selection and value estimation processes. The update rule is: \\[\nQ_{1}(s_{t}, a_{t}) \\leftarrow Q_{1}(s_{t}, a_{t}) + \\alpha(R_{t+1} + \\gamma Q_{2}(s_{t + 1}, \\underset{a'}{\\operatorname{arg\\max}\\ }Q_{1}(s_{t+1}, a') ) - Q_{1}(s_{t}, a_{t}))\n\\]\nThe Loss Function for the Double Deep Q_Learning is that: \\[\n\\mathcal{L}(\\theta)=\\mathbb{E}[(R_{t+1} + \\gamma Q(s_{t+1}, \\underset{a'}{\\operatorname{arg\\max}\\ }Q(s_{t+1}, a'; \\theta);\\theta^{-} )-Q(s_{t}, a_{t}; \\theta))^{2}]\n\\]"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#dueling-dqn",
    "href": "posts/Overviews/Overview_DeepRL.html#dueling-dqn",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Dueling DQN",
    "text": "Dueling DQN\nThe problem of the Basic DQN is that not all actions are equally relevant in every state, but DQN treats all state-action equally, to solve this problem, we can use Dueling DQN (Wang et al. 2016), which separates the Q-value into two components:\n\nState-Value function \\(V(s)\\): measures the value of being in state \\(s\\)\nAdvantage function \\(A(s, a)\\), which measures the relative importance of taking action \\(a\\) in state \\(s\\).\n\nThe Q-value is them computed as: \\[\nQ(s, a) = V(s) + A(s, a) - \\underset{a'}{\\max \\ } A(s, a')\n\\]"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#noisy-q-learning",
    "href": "posts/Overviews/Overview_DeepRL.html#noisy-q-learning",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Noisy Q-Learning",
    "text": "Noisy Q-Learning\nIn some environments, the agents needs better exploration strategies to avoid getting suck in local optima. The Noisy-Q-Network (Fortunato et al. 2019) introduces parametric noise into the neural network’s parameters, allowing the agent to explore more effectively"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#soft-q-learning",
    "href": "posts/Overviews/Overview_DeepRL.html#soft-q-learning",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Soft Q-Learning",
    "text": "Soft Q-Learning\nSoft Q-learning(Haarnoja et al. 2017) introduces an entropy term into the objective to encourage exploration. The agent seeks to maximize both the cumulative reward and the policy entropy, learning yo better exploration in uncertain environments\nThe Objective Function become: \\[\nJ(\\pi) =\\mathbb{E}\\left[ \\sum_{t}(R_{t} + \\alpha \\mathcal{H}(\\pi(\\cdot | s_{t}))) \\right]\n\\]"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#experience-replay-buffer",
    "href": "posts/Overviews/Overview_DeepRL.html#experience-replay-buffer",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Experience Replay Buffer",
    "text": "Experience Replay Buffer\nOne of the vital component in the RL toolkit that boosts learning and stable is the training is the Experience Replay Buffer. There are several reason to use the Experience Replay Buffer:\n\nImproved Sample Efficiency: Collecting new experience constantly is costly and inefficient. By storing past experiences, the agent can use each iteration multiple times\nBreaking Temporal Correlations: Learning sequentially from consecutive experiences introduces high correlations, which destabilize the neural network’s updates.1\nStabilizing Learning: Replay buffers ensure that the training data is diverse, including experiences from various states and actions. This helps prevent overfitting to recent experiences and ensures that rare but valuable experiences (like achieving a goal or encountering a penalty) are reused and learned from.\nHandling Sparse Rewards: In some environments, rewards are sparse or rare.\n\n\nBasic Experienc Replay Buffer\nThe basic replay buffer is a list contain a tuple of \\((s, a, r, s', d)\\), which captures one interaction in the environment.\n\n\nPrioritized Experience Replay(PER)\nPrioritized Experience Replay enhances the standard buffer by focusing on experiences that are more informative. Experiences that yield higher learning signals (or TD errors) are prioritized because they represent areas where the agent has the most to learn."
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#reinforce",
    "href": "posts/Overviews/Overview_DeepRL.html#reinforce",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "REINFORCE",
    "text": "REINFORCE\nREINFORCE is the most basic algorithm used in the reinforcement learning algorithms. The basic process is:\n\nSample trajectory \\(\\{ \\tau^{i}\\}\\) according to policy \\(\\pi_{\\theta}\\)\nCalculate the gradient of the \\(J(\\theta^{-})\\)\nUpdate the parameters: \\[\n\\theta \\leftarrow  \\theta^{-} + \\alpha \\nabla_{\\theta^{-}}J(\\theta^{-})\n\\]\n\n\nREINFORCE with Baseline\nThe baseline, we can make that the positive reward increase when the bad reward decrease, the basic baseline is: \\[\nb = \\frac{1}{N}\\sum_{i=1}^{N}r(\\tau)\n\\] So, the objective function become: \\[\n\\nabla_{\\theta}J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^{N}\\nabla _{\\theta}\\log p_{\\theta}(\\tau)[r(\\tau) - b]\n\\]\n\n\n\n\n\n\nWhy can we add baseline\n\n\n\nWe can adding a baseline \\(b\\) to the gradient because it does not affect the expected value of the policy gradient:\n\\[\n\\mathbb{E}[\\nabla_\\theta \\log p_\\theta (\\tau) b] = \\int p_\\theta (\\tau) \\nabla_\\theta \\log p_\\theta (\\tau) b \\, d\\tau = \\int \\nabla_\\theta p_\\theta (\\tau) b \\, d\\tau = b \\nabla_\\theta \\int p_\\theta (\\tau) \\, d\\tau = b \\nabla_\\theta 1 = 0\n\\]\nSubtracting a baseline is unbiased in expectation.\n\n\nA common choice for the baseline is the value function \\(V(s)\\).\n\n\nOff-Policy REINFORCE\n\\[\n\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[\\nabla \\log p_{\\theta}(\\tau)r(\\tau)]\n\\] The way we calculate the objective function is by sampling trajectories using the latest policy, so need throw out the samples each time we change \\(\\theta\\). We cannot retain samples from previous policy.\nHowever neural network change only a little bit with each gradient step, so this might be sample expensive. So, we can transfer this into off-policy learning through important sampling.\n\n\nNatural Gradient\nSome parameters change probabilities a lot more than others, what we want to by changing the learning rate for different parameters, that have larger learning rate for those parameters that don’t change policy very much, in order to all the parameters to reach optimal value. Look constraints optimization view of first order graident descent,.\nFrom mirror descent or projected gradient descent.  Usually just take \\(\\alpha\\) rather then \\(\\epsilon\\) where \\(\\alpha\\) is just the lagrange multiplier that corresponds to epsilon.\nThe step size are equal size in policy space rather than rather than parameter space. which means rescale the gradients\nConjugate gradient works well for this."
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#actor-critic-methods",
    "href": "posts/Overviews/Overview_DeepRL.html#actor-critic-methods",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Actor-Critic Methods",
    "text": "Actor-Critic Methods\n\nBasic Actor-Critic Algorithm\n\n\nAdvantage Actor-Critic Algorithm\n\n\nAsyn Advantage Actor-Critic Algorithm (A3C)"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#trust-region-policy-optimization",
    "href": "posts/Overviews/Overview_DeepRL.html#trust-region-policy-optimization",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Trust Region Policy Optimization",
    "text": "Trust Region Policy Optimization\nmotivated by the natural gradient but includes additional constraints for robustness"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#proximal-policy-optimization",
    "href": "posts/Overviews/Overview_DeepRL.html#proximal-policy-optimization",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Proximal Policy Optimization",
    "text": "Proximal Policy Optimization\nThis is an simpler alternative to TRPO which approximates the natural gradient updates through a simpler, more efficient objective function."
  },
  {
    "objectID": "posts/Overviews/Overview_DeepGenerativeModel.html",
    "href": "posts/Overviews/Overview_DeepGenerativeModel.html",
    "title": "Overview: Deep Generative Models",
    "section": "",
    "text": "Overview of the Deep Generative Models"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuyang’s Blog",
    "section": "",
    "text": "Improvements of Transformer Model\n\n\n\n\n\n\nPaper\n\n\nCode Implementation\n\n\n\nThis is my understanding of the paper Attention is all you need. This paper introduces the most important model in the Deep Learning field: Transformer. The Transformer is the corner stone of current Large Language Model. This blog includes theory and implmentation of the Transformer using Pytorch from scratch.\n\n\n\n\n\nNov 14, 2023\n\n\n1 min\n\n\n165 words\n\n\n11/26/24, 5:02:41 PM\n\n\n\n\n\n\n\n\n\n\n\n\nDive into: Attention is all you need\n\n\n\n\n\n\nPaper\n\n\nCode Implementation\n\n\nNLP\n\n\n\nThis is my understanding of the paper Attention is all you need. This paper introduces the most important model in the Deep Learning field: Transformer. The Transformer is the corner stone of current Large Language Model. This blog includes theory and implmentation of the Transformer using Pytorch from scratch.\n\n\n\n\n\nOct 5, 2023\n\n\n3 min\n\n\n564 words\n\n\n11/26/24, 4:49:02 PM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: (Multimodal) Large Language Models\n\n\n\n\n\n\nLLM\n\n\nOverview\n\n\n\nThis is an overview of the Muultimodal Large Language Models. It includes the defintion of the large language models, the multimodal large language models, and the applications of those models. Besides, it also includes to fine-tune those models with different methods like LoRA.\n\n\n\n\n\nOct 25, 2024\n\n\n2 min\n\n\n388 words\n\n\n11/5/24, 10:35:16 PM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Deep Reinforcement Learning\n\n\n\n\n\n\nReinforcement Learning\n\n\nOverview\n\n\n\nThis is an overview of the Deep Reinforcement Learning. It includes the defintion of the deep reinforcement learning, some popular algorithms for example deep Q learning and variants, policy gradient methods and actor-critic methods for example A3C, DDPG, and PPO. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n12 min\n\n\n2,241 words\n\n\n10/31/24, 5:42:54 PM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Optimization Algorithms\n\n\n\n\n\n\nOptmization\n\n\nOverview\n\n\n\nThis is an overview of the Optmization Algorithms. It includes the definition of the optimization algorithms, the most popular optimization algorithms like first-order optmization(e.g. SGD), higher-order optmization(e.g. Newton’s Method) algorithms. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n6 min\n\n\n1,161 words\n\n\n10/31/24, 1:03:20 PM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Probabilistic Graphical Models\n\n\n\n\n\n\nProbability\n\n\nOverview\n\n\n\nThis is an overview of Probabilistic Graphical Models. It includes the definition of the Probabilistic Graphical Models, different types of PGMs like Bayesian Networks, Markov Networks, and the different inference algorithms like Variable Elimination, Gibbs Sampling, and Belief Propagation, Variance Inference. Also, it includes the learning methods like Maximum Likelihood Estimation, Maximum A Posteriori Estimation, and Expectation Maximization. \n\n\n\n\n\nOct 25, 2024\n\n\n12 min\n\n\n2,308 words\n\n\n10/31/24, 12:17:31 AM\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Markov Chain to Markov Decision Process and MCMC\n\n\n\n\n\n\nBasic Techniques\n\n\nReinforcement Learning\n\n\n\nThis blog explores the Markov Chain, Markov Decision Process, and Markov Chain Monte Carlo(MCMC) algorithm. It covers the definition of Markov Chain, the mathematical foundation of Markov Decision Process, and the application of MCMC in Bayesian Inference.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n10 words\n\n\n10/28/24, 12:43:18 AM\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Learning: From Basic to Advance\n\n\n\n\n\n\nBasic Techniques\n\n\nLearning\n\n\n\nThis blog explores Maximum Likelihhood Estimation(MLE) and its applications across machine learning, from basic densitty estimation to deep learning. It also discuess optmization techniques like SGD, when MLE lacks a closed-from soluition, which is most common in deep learning.\n\n\n\n\n\nOct 26, 2024\n\n\n8 min\n\n\n1,417 words\n\n\n10/27/24, 11:23:43 PM\n\n\n\n\n\n\n\n\n\n\n\n\nExpectation Maximization\n\n\n\n\n\n\nBasic Techniques\n\n\nLearning\n\n\n\nThis blog explores the Expectation Maximization(EM) algorithm, a powerful technique for estimating parameters in latent variable models. It covers the intuition behind EM, its applications in clustering and Gaussian Mixture Models(GMM), and how it overcomes the challenges of missing data.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n21 words\n\n\n10/27/24, 2:39:02 AM\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Foundations and Concepts: Chapter 03 Summary and Exercise Solutions\n\n\n\n\n\n\nDLFC\n\n\n\nThis is my understanding of the paper Attention is all you need. This paper introduces the most important model in the Deep Learning field: Transformer. The Transformer is the corner stone of current Large Language Model. This blog includes theory and implmentation of the Transformer using Pytorch from scratch.\n\n\n\n\n\nOct 24, 2024\n\n\n1 min\n\n\n19 words\n\n\n10/25/24, 10:49:07 AM\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Foundations and Concepts: Chapter 02 Summary and Exercise Solutions\n\n\n\n\n\n\nDLFC\n\n\n\nThis is my understanding of the paper Attention is all you need. This paper introduces the most important model in the Deep Learning field: Transformer. The Transformer is the corner stone of current Large Language Model. This blog includes theory and implmentation of the Transformer using Pytorch from scratch.\n\n\n\n\n\nNov 26, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 2:58:33 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Deep Generative Models\n\n\n\n\n\n\nGenerative Model\n\n\nOverview\n\n\n\nThis is an overview of the Deep Generative Models. It includes the most popular generative models like VAE, GAN, and Flow-based models. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 12:43:00 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Deep Meta Learning\n\n\n\n\n\n\nMeta Learning\n\n\nOverview\n\n\n\nThis is an overview of the Meta Learning. It includes the definition of the multi-task learning, transfer learning and meta learning. Three most common types of meta-learning algorithms, metric learning, model-based learning, and optimization-based learning are also included. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 12:42:54 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#conjugate-gradientcg",
    "href": "posts/Overviews/Overview_Optimization.html#conjugate-gradientcg",
    "title": "Overview: Optimization Algorithms",
    "section": "Conjugate Gradient(CG)",
    "text": "Conjugate Gradient(CG)\nThe conjugate gradient method is a method that only needs first-order derivative information for well-defined quadratic programming, but overcomes the shortcoming of steepest descent method, and avoids the disadvantages of Newton’s method of storing and calculating the inverse Hessian matrix.\nIt is one of the most effective methods for solving large-scale linear systems of equations. It can also be used for solving non-linear optimization equations. In the 1960s, CG method is proposed as an alternative to Gaussian elimination.\nFor a linear system: \\[\nA\\theta = b\n\\tag{1}\\] where \\(A \\in \\mathbb{S}^{n \\times n}_{++}\\) is symmetric, positive-definite matrix. The matrix \\(A\\) and vector \\(b\\) are known, and we want to solve the value of \\(\\theta\\). The problem of minimize the quadratic positive definite function:\n\\[\n\\underset{\\theta}{\\min \\ } F(\\theta) = \\frac{1}{2}\\theta^{T}A\\theta - b\\theta + c\n\\] can be seen as a linear system when we set the gradient of \\(F(\\theta^{*}) = 0\\): \\[\n\\frac{d F(\\theta)}{d\\theta} = A\\theta - b\n\\] {#1} If \\(A\\) is symmetric matrix.\nThe gradient of \\(F(\\theta)\\) can be obtained by simple calculation which is equals to the residual of the linear system: \\[\n\\nabla F(\\theta) = r(\\theta) = A\\theta - b\n\\]\nThe conjugate is defined as: Given an matrix \\(A \\in \\mathbb{S}^{n\\times n}_{++}\\), two non-zero vector \\(d_{i}, d_{j}\\) are conjugate with respect to \\(A\\) if: \\[\nd_{i}^{T}Ad_{j} = 0\n\\] This is also called as \\(A\\)-orthogonality.\nA set of non-zero vector \\(\\{ d_{1}, d_{2}, \\dots, d_{n}\\}\\) is said to be conjugate with respect to A if any unequal vectors are conjugate with respect to \\(A\\).\nTo derive the conjugate gradient method, we first initialize some random start point \\(\\theta_{0}\\) as starting point. And update the \\(\\theta\\) according to: \\[\n\\theta_{t + 1} = \\theta_{t} + \\eta_{t}d_{t}\n\\] The step size \\(\\eta_{t}\\) can be obtained by a linear search, which means choose \\(\\eta_{t}\\) to minimize the objective function \\(f(\\cdot)\\) along \\(\\theta_{t} + \\eta_{t}d_{t}\\). The formula of \\(\\eta_{t}\\) is: \\[\n\\eta_{t} = \\frac{r_{t}^{\\intercal}r_{t}}{d^{\\intercal}Ad_{t}}\n\\]\nThe search direction \\(d_{t}\\) is obtained by a linear combination of negative residual and the previous search direction: \\[\nd_{t} = - r_{t} +\\beta_{t}d_{t-1}\n\\] Where the \\(\\beta_{t}\\) is the update parameter, determined by: \\[\n\\beta_{t} = \\frac{r_{t}^{\\intercal}r_{t}}{r^{\\intercal}_{t-1}r_{t-1}}\n\\]\nOne of the add property of the CG method is that to generating a new vector \\(d_{t}\\), we know need previous vector \\(d_{t-1}\\)\nCheck this for further illustrate : An Introduction to the Conjugate Gradient Method Without the Agonizing Pain"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#linear-programming",
    "href": "posts/Overviews/Overview_Optimization.html#linear-programming",
    "title": "Overview: Optimization Algorithms",
    "section": "Linear Programming",
    "text": "Linear Programming"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#quadratic-programming",
    "href": "posts/Overviews/Overview_Optimization.html#quadratic-programming",
    "title": "Overview: Optimization Algorithms",
    "section": "Quadratic Programming",
    "text": "Quadratic Programming"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#semi-definition-programming",
    "href": "posts/Overviews/Overview_Optimization.html#semi-definition-programming",
    "title": "Overview: Optimization Algorithms",
    "section": "Semi-Definition Programming",
    "text": "Semi-Definition Programming"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#quasi-newton-methods",
    "href": "posts/Overviews/Overview_Optimization.html#quasi-newton-methods",
    "title": "Overview: Optimization Algorithms",
    "section": "Quasi-Newton Methods",
    "text": "Quasi-Newton Methods\nThe basic idea of Newton’s method is to use both the first-order derivative and second order derivative to approximate the objective function with a quadratic function, and then solve the minimum optimization of the quadratic function. The one-dimensional Newton’s iteration formula is shown as: \\[\n\\theta_{t+1}= \\theta_{t} - \\frac{f'(\\theta_{t})}{f''(\\theta_{t})}\n\\] The high dimensional Newton’s iteration formula is: \\[\n\\theta_{t+1} = \\theta_{t} - \\nabla^{2}f(\\theta_{t})^{-1}\\nabla f(\\theta_{t})\n\\] {#1}\nwhere \\(\\nabla^{2} f\\) is Hessian matrix. The above method is called damping Newton’s method.\nThe problem of this method is computing inverse of the Hessian matrix at every iterations, which makes the storage and computation very expensive. To solve the problem we can approximate the inverse of the Hessian matrix, which is called quasi-Newton method.\nIt use positive definite matrix to approximate the inverse of the Hessian matrix. More specifically, the second-order gradient is not directly needed in this method. There are several methods to approximate the Hessian matrix and inverse matrix: ### Quasi-Newton Condition Assume the objective function \\(f\\) can be approximated by a quadratic function, we can extend \\(f(\\theta)\\) to Taylor series at \\(\\theta =\\theta_{t+1}\\): \\[\nf(\\theta) \\approx f(\\theta_{t+1}) + \\nabla f(\\theta_{t+1})^{\\intercal}(\\theta - \\theta_{t+1}) + \\frac{1}{2}(\\theta - \\theta_{t+1})^{\\intercal}\\nabla^{2}f(\\theta_{t+1})(\\theta - \\theta_{t+1})\n\\]\nWe can compute the gradient on both sides of the above equation and obtain: \\[\n\\nabla f(\\theta) \\approx \\nabla f(\\theta_{t+1}) + \\nabla^{2}f(\\theta_{t+1})(\\theta - \\theta_{t+!})\n\\] Let set \\(\\theta = \\theta_{t}\\), we have: \\[\n\\nabla f(\\theta_{t}) \\approx \\nabla f(\\theta_{t+1}) + \\nabla^{2}f(\\theta_{t+1})(\\theta_{t} - \\theta_{t+1})\n\\] We use \\(B\\) to represent the approximate matrix of the Hessian matrix. Set \\(s_{t} = \\theta_{t+1} - \\theta_{t}\\) and \\(u_{t} = \\nabla f(\\theta_{t+1}) - \\nabla f(\\theta_{t})\\)\nThe matrix \\(B_{t+1}\\) is satisfied that: \\[\nu_{t} = B_{t+1} s_{t}\n\\] The above equation is called the quasi-Newton condition, or secant equation. The search direction of quasi-Newton method is: \\[\nd_{t} = -B_{t}^{-1}g_{t}\n\\tag{2}\\] where \\(g_{t}\\) is the gradient of \\(f\\), and the update of quasi-Newton is: \\[\n\\theta_{t+1} = \\theta_{t} + \\eta_{t}d_{t}\n\\] The step size \\(\\eta_{t}\\) is chose to satisfy the Wolfe conditions.\n\n\nWolfe conditions: a set of inequalities for inexact line searches \\(\\underset{\\eta_{t}}{\\min \\ }f(\\theta_{t} + \\eta_{t} d_{t})\\)\n\nDFP\n\n\n\nimage.png\n\n\n\n\nBFGS\n\n\n\nimage.png\n\n\n\n\nL-BFGS\nLimited memory quasi-Newton methods, named L-BFGS is an improvement based on the quasiNewton method. which is feasible in dealing with the highdimensional situation.\n\ndas\ndasd\n\ndas\n\ndas d\n\nhttps://towardsdatascience.com/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504 ## Stochastic Quasi-Newton Method"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#hessian-free-optimization-methods",
    "href": "posts/Overviews/Overview_Optimization.html#hessian-free-optimization-methods",
    "title": "Overview: Optimization Algorithms",
    "section": "Hessian-Free Optimization Methods",
    "text": "Hessian-Free Optimization Methods"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#natural-gradient",
    "href": "posts/Overviews/Overview_Optimization.html#natural-gradient",
    "title": "Overview: Optimization Algorithms",
    "section": "Natural Gradient",
    "text": "Natural Gradient"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html#trust-region-method",
    "href": "posts/Overviews/Overview_Optimization.html#trust-region-method",
    "title": "Overview: Optimization Algorithms",
    "section": "Trust Region Method",
    "text": "Trust Region Method"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#algorithms",
    "href": "posts/Overviews/Overview_DeepRL.html#algorithms",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Algorithms",
    "text": "Algorithms\nMost algorithm can be separated into three parts: \n\nGenerated samples: samples scan be generated from same or different policy, this is through trial-and-error\nFit and a model / estimate the return:\n\nCan be fit as the evauated as the cumulative rewards(model free)\nCan fit the model of the environment(model base): \\[\ns_{t+1} \\approx f_{\\phi}(s_{t}, a_{t})\n\\]\n\nImprove the policy: use some optimization algorithms(e.g. gradient ascent) to optimize the parameters\n\nThe orange can be expensive:\n\nIn real world: this is expensive because we can not speed up the sampling process\nIn simulator: this is much cheap.\n\nThe green steps can also be expensive or cheap:\n\nJust sum over the whole reward is cheap\nTry to get the model of the environment will be expensive.\n\nThe Blue also has different cost:\n\nJust update the parameters will be cheap.\nUpdate the parameters according to the model and policy will be expensive.\n\nThere are different types of algorithms:\n\nPolicy Gradient\nValue-Based\nActor-Critic\nModel-Based:\n\nFor Planning\nFor improve policy\n\n\nNow, let’s introduce some specific algorithms, we first start with Deep Q Network, which was first introduced by Mnih et al. (2013)"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#footnotes",
    "href": "posts/Overviews/Overview_DeepRL.html#footnotes",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is because the model may “overfitting” on recent patterns or states, making it difficult to generalize across states↩︎\nCausality: policy at time \\(t'\\) cannot affect reward at time \\(t\\) when \\(t &lt; t'\\)↩︎"
  },
  {
    "objectID": "posts/Overviews/Overview_(M)LLM.html",
    "href": "posts/Overviews/Overview_(M)LLM.html",
    "title": "Overview: (Multimodal) Large Language Models",
    "section": "",
    "text": "New content"
  },
  {
    "objectID": "posts/Overviews/Overview_(M)LLM.html#causal-decoder-architecture",
    "href": "posts/Overviews/Overview_(M)LLM.html#causal-decoder-architecture",
    "title": "Overview: (Multimodal) Large Language Models",
    "section": "Causal Decoder Architecture",
    "text": "Causal Decoder Architecture\nThe causal decoder architecture incorporates the unidirectional attention mask, to guarantee that each input token can only attend to the past tokens and itself.\n\n\nSee my implementation of GPT2 as this link.\nAnd my implementation of LLaMa as this link.\nThe GPT-series models are developed based on the causal-decoder architecture. It show amazing ability in in-context learning capability of LLMs."
  },
  {
    "objectID": "posts/Overviews/Overview_(M)LLM.html#prefix-decoder-architecture",
    "href": "posts/Overviews/Overview_(M)LLM.html#prefix-decoder-architecture",
    "title": "Overview: (Multimodal) Large Language Models",
    "section": "Prefix Decoder Architecture",
    "text": "Prefix Decoder Architecture\nNon like Causal Decoder Architecture, the prefix decoder architecture enable performing bi-directional attention over the prefix tokens and unidirectional attention only on generated tokens. In this way, like the encoder-decoder architecture, the prefix decoders can bidirectionally encode the prefix sequence and auto-regressively predict the output tokens one by one. Usually, a practical suggestion is to continually train causal decoders and then convert them into prefix decoder for accelerating convergence."
  },
  {
    "objectID": "posts/Overviews/Overview_(M)LLM.html#mixture-of-experts",
    "href": "posts/Overviews/Overview_(M)LLM.html#mixture-of-experts",
    "title": "Overview: (Multimodal) Large Language Models",
    "section": "Mixture of Experts",
    "text": "Mixture of Experts\nWe can further extend the above architecture via the mixture of experts (MOE ) scaling. In which a subset of neural network weights for each input are sparely activated, the example of Switch Transformer.\n\n\nSee my implementation of Switch Transformer as this link.\nThe major merit is that MoE is a flexible way to scale up the model parameter while maintaining a constant computational cost. Is has been shown that substantial performance improvement can be observed by increasing either the number of experts. or the total parameter size. It is widespread speculation that GPT-4 has been developed based on the MoE architecture."
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#section",
    "href": "posts/Overviews/Overview_DeepRL.html#section",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "",
    "text": "Hindsight Experience Replay (HER)\nHindsight Experience Replay is designed for environments where rewards are sparse, such as goal-directed tasks. In HER, the agent reinterprets experiences as if the achieved outcome was intended, enabling it to learn even from unsuccessful episodes by using alternate goals."
  },
  {
    "objectID": "posts/Overviews/Overview_(M)LLM.html#encoder-decoder-architecture",
    "href": "posts/Overviews/Overview_(M)LLM.html#encoder-decoder-architecture",
    "title": "Overview: (Multimodal) Large Language Models",
    "section": "Encoder Decoder Architecture",
    "text": "Encoder Decoder Architecture\nThe vanilla Transformer(Vaswani et al. (2023)) model is based on that architecture.\n\n\nSee my implementation of transformer as this link .\nThe encoder adopts stacked multi-head self-attention layers to encoder the input sequence for generating its latent representation, the decoder perform cross-attention on these representations, while decoder performs cross-attention on theses representation and auto-regressively generates the target sequence.\nSo far, there are only a small number of LLMs that are built based on the encoder-decoder architecture."
  },
  {
    "objectID": "posts/Overviews/Overview_(M)LLM.html#emergent-architectures",
    "href": "posts/Overviews/Overview_(M)LLM.html#emergent-architectures",
    "title": "Overview: (Multimodal) Large Language Models",
    "section": "Emergent Architectures",
    "text": "Emergent Architectures\nTo improve the efficiency, there is new architectures have been proposed based on the parameterized state space models, which can be viewed as a combination of RNN and CNN. However, there performance still lags behind Transformer. Thus several variants of SSM have been proposed, such as Mamba\n\n\n\n\n\nSee my implementation of Mamba as this link."
  },
  {
    "objectID": "posts/Papers/Attention_is_all_you_need.html#encoder",
    "href": "posts/Papers/Attention_is_all_you_need.html#encoder",
    "title": "Dive into: Attention is all you need",
    "section": "Encoder",
    "text": "Encoder"
  },
  {
    "objectID": "posts/Papers/Attention_is_all_you_need.html#decoder",
    "href": "posts/Papers/Attention_is_all_you_need.html#decoder",
    "title": "Dive into: Attention is all you need",
    "section": "Decoder",
    "text": "Decoder"
  },
  {
    "objectID": "posts/Papers/Attention_is_all_you_need.html#self-attention",
    "href": "posts/Papers/Attention_is_all_you_need.html#self-attention",
    "title": "Dive into: Attention is all you need",
    "section": "Self-Attention",
    "text": "Self-Attention"
  },
  {
    "objectID": "posts/Papers/Attention_is_all_you_need.html#multi-head-attention",
    "href": "posts/Papers/Attention_is_all_you_need.html#multi-head-attention",
    "title": "Dive into: Attention is all you need",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention"
  },
  {
    "objectID": "posts/Papers/Attention_is_all_you_need.html#optimizer",
    "href": "posts/Papers/Attention_is_all_you_need.html#optimizer",
    "title": "Dive into: Attention is all you need",
    "section": "Optimizer",
    "text": "Optimizer"
  },
  {
    "objectID": "posts/Papers/Attention_is_all_you_need.html#lable-smooting",
    "href": "posts/Papers/Attention_is_all_you_need.html#lable-smooting",
    "title": "Dive into: Attention is all you need",
    "section": "Lable Smooting",
    "text": "Lable Smooting"
  },
  {
    "objectID": "posts/Basic/Transformer_Imrpoved.html",
    "href": "posts/Basic/Transformer_Imrpoved.html",
    "title": "Improvements of Transformer Model",
    "section": "",
    "text": "Transformer is the one of the most important architecture in the deep learning. To get what is the transformer, check this blog. However, there are several improvement has been proposed after the transformer to improve the training efficient and model performs. In the article, I’m going to get in to some details of those improvements. Basicaly from belowing aspects:\n\nPosition Encoding: Current Position Encoding is the absolute position encoding, it cannot learn the relative position\nAttention: As the most important part in the Transformer, it is also the most computing required blocks\nContext Length: The basic transformer model, the context length is fixed\nNormalization: Current Normalization is the Layer Normalization, is there are methods that can learn the"
  },
  {
    "objectID": "posts/Basic/Transformer_Imrpoved.html#sinusoidal-positional-encoding",
    "href": "posts/Basic/Transformer_Imrpoved.html#sinusoidal-positional-encoding",
    "title": "Improvements of Transformer Model",
    "section": "Sinusoidal Positional Encoding",
    "text": "Sinusoidal Positional Encoding"
  },
  {
    "objectID": "posts/Basic/Transformer_Imrpoved.html#relative-position-encoding",
    "href": "posts/Basic/Transformer_Imrpoved.html#relative-position-encoding",
    "title": "Improvements of Transformer Model",
    "section": "Relative Position Encoding",
    "text": "Relative Position Encoding"
  },
  {
    "objectID": "posts/Basic/Transformer_Imrpoved.html#learned-positional-encoding",
    "href": "posts/Basic/Transformer_Imrpoved.html#learned-positional-encoding",
    "title": "Improvements of Transformer Model",
    "section": "Learned Positional Encoding",
    "text": "Learned Positional Encoding"
  },
  {
    "objectID": "posts/Basic/Transformer_Imrpoved.html#rotary-position-embedding",
    "href": "posts/Basic/Transformer_Imrpoved.html#rotary-position-embedding",
    "title": "Improvements of Transformer Model",
    "section": "Rotary Position Embedding",
    "text": "Rotary Position Embedding"
  },
  {
    "objectID": "posts/Basic/Transformer_Imrpoved.html#linear-attention",
    "href": "posts/Basic/Transformer_Imrpoved.html#linear-attention",
    "title": "Improvements of Transformer Model",
    "section": "Linear Attention",
    "text": "Linear Attention"
  },
  {
    "objectID": "posts/Basic/Transformer_Imrpoved.html#sparse-attention",
    "href": "posts/Basic/Transformer_Imrpoved.html#sparse-attention",
    "title": "Improvements of Transformer Model",
    "section": "Sparse Attention",
    "text": "Sparse Attention"
  },
  {
    "objectID": "posts/Basic/Transformer_Imrpoved.html#grouped-query-attention-multi-query-attention",
    "href": "posts/Basic/Transformer_Imrpoved.html#grouped-query-attention-multi-query-attention",
    "title": "Improvements of Transformer Model",
    "section": "Grouped Query Attention / Multi-Query Attention",
    "text": "Grouped Query Attention / Multi-Query Attention"
  },
  {
    "objectID": "posts/Basic/Transformer_Imrpoved.html#flash-attention",
    "href": "posts/Basic/Transformer_Imrpoved.html#flash-attention",
    "title": "Improvements of Transformer Model",
    "section": "Flash Attention",
    "text": "Flash Attention"
  },
  {
    "objectID": "posts/Basic/Transformer_Imrpoved.html#kv-cache",
    "href": "posts/Basic/Transformer_Imrpoved.html#kv-cache",
    "title": "Improvements of Transformer Model",
    "section": "KV Cache*",
    "text": "KV Cache*"
  }
]