[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a master student of Singapore Management University majoring in IT in Business with AI track. I am passionate about AI. I have a strong background in computer science and big data. I am proficient in Python. I have experience in machine learning, deep learning. I am a quick learner and a good team player. I like to learn new things summary and show them with each other. Currently I’m looking for a full-time job in AI field."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nSingapore Management University | Singapre Master of IT in Business(AI track) | Sept 2023 -\nUniversity of Wollongong | Computer Science B.S in Big Data | March 2020 - March 2023"
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nMachine Learning & Deep Learning\nComputer Vision\nNatural Language Processing\nReinforcement Learning\nMeta Learning\nGenerative Model\nUnsupervised Learning\nLarge Language Model\nConvex Optmization\nProbabilitic Graphical Model\nDeep Graph Learning"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About Me",
    "section": "Projects",
    "text": "Projects"
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html",
    "href": "posts/Overviews/Overview_PGM.html",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "",
    "text": "Overview of Probabilistic Graphical Models"
  },
  {
    "objectID": "posts/Overviews/Overview_MetaLearning.html",
    "href": "posts/Overviews/Overview_MetaLearning.html",
    "title": "Overview: Deep Meta Learning",
    "section": "",
    "text": "Overview of the Deep Meta Learning"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html",
    "href": "posts/Overviews/Overview_Optimization.html",
    "title": "Overview: Optimization Algorithms",
    "section": "",
    "text": "Overview of Optimization Algorithms"
  },
  {
    "objectID": "posts/Readings/DLFC/Chapter02.html",
    "href": "posts/Readings/DLFC/Chapter02.html",
    "title": "Deep Learning Foundations and Concepts: Chapter 02 Summary and Exercise Solutions",
    "section": "",
    "text": "Deep Learning Foundations and Concepts Chapter 02 Summary and Exercise Solutions"
  },
  {
    "objectID": "posts/Papers/Attention_is_all_you_need.html",
    "href": "posts/Papers/Attention_is_all_you_need.html",
    "title": "Dive into: Attention is all you need",
    "section": "",
    "text": "Paper Reading: Attention is all you need\n\n\nReference"
  },
  {
    "objectID": "posts/Readings/DLFC/Chapter03.html",
    "href": "posts/Readings/DLFC/Chapter03.html",
    "title": "Deep Learning Foundations and Concepts: Chapter 03 Summary and Exercise Solutions",
    "section": "",
    "text": "This is the content of what I learning in the DLFC.\nHighlights of thehioho\ndas a a da"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html",
    "href": "posts/Overviews/Overview_DeepRL.html",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "",
    "text": "Overview of the Deep Reinforcement Learning\nasdasd\nsdasd"
  },
  {
    "objectID": "posts/Overviews/Overview_(M)LLM.html",
    "href": "posts/Overviews/Overview_(M)LLM.html",
    "title": "Overview: (Multimodal) Large Language Models",
    "section": "",
    "text": "Overview of the Multimodal Large Language Models"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepGenerativeModel.html",
    "href": "posts/Overviews/Overview_DeepGenerativeModel.html",
    "title": "Overview: Deep Generative Models",
    "section": "",
    "text": "Overview of the Deep Generative Models"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuyang’s Blog",
    "section": "",
    "text": "Maximum Likelihood Estimation: From Basic to Advance\n\n\n\n\n\n\nBasic Techniques\n\n\n\nThis blog explores Maximum Likelihhood Estimation(MLE) and its applications across machine learning, from basic densitty estimation to deep learning. It also discuess optmization techniques like SGD, when MLE lacks a closed-from soluition, which is most common in deep learning.\n\n\n\n\n\nOct 26, 2024\n\n\n3 min\n\n\n478 words\n\n\n10/26/24, 3:19:21 AM\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Foundations and Concepts: Chapter 02 Summary and Exercise Solutions\n\n\n\n\n\n\nDLFC\n\n\n\nThis is my understanding of the paper Attention is all you need. This paper introduces the most important model in the Deep Learning field: Transformer. The Transformer is the corner stone of current Large Language Model. This blog includes theory and implmentation of the Transformer using Pytorch from scratch.\n\n\n\n\n\nOct 26, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 2:58:33 AM\n\n\n\n\n\n\n\n\n\n\n\n\nDive into: Attention is all you need\n\n\n\n\n\n\nPaper\n\n\nCode Implementation\n\n\n\nThis is my understanding of the paper Attention is all you need. This paper introduces the most important model in the Deep Learning field: Transformer. The Transformer is the corner stone of current Large Language Model. This blog includes theory and implmentation of the Transformer using Pytorch from scratch.\n\n\n\n\n\nOct 26, 2024\n\n\n1 min\n\n\n3 words\n\n\n10/25/24, 2:56:35 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Probabilistic Graphical Models\n\n\n\n\n\n\nProbability\n\n\nOverview\n\n\n\nThis is an overview of Probabilistic Graphical Models. It includes the definition of the Probabilistic Graphical Models, different types of PGMs like Bayesian Networks, Markov Networks, and the different inference algorithms like Variable Elimination, Gibbs Sampling, and Belief Propagation, Variance Inference. Also, it includes the learning methods like Maximum Likelihood Estimation, Maximum A Posteriori Estimation, and Expectation Maximization. \n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 12:43:09 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Optimization Algorithms\n\n\n\n\n\n\nOptmization\n\n\nOverview\n\n\n\nThis is an overview of the Optmization Algorithms. It includes the definition of the optimization algorithms, the most popular optimization algorithms like first-order optmization(e.g. SGD), higher-order optmization(e.g. Newton’s Method) algorithms. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 12:43:06 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Deep Reinforcement Learning\n\n\n\n\n\n\nReinforcement Learning\n\n\nOverview\n\n\n\nThis is an overview of the Deep Reinforcement Learning. It includes the defintion of the deep reinforcement learning, some popular algorithms for example deep Q learning and variants, policy gradient methods and actor-critic methods for example A3C, DDPG, and PPO. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 12:43:03 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Deep Generative Models\n\n\n\n\n\n\nGenerative Model\n\n\nOverview\n\n\n\nThis is an overview of the Deep Generative Models. It includes the most popular generative models like VAE, GAN, and Flow-based models. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 12:43:00 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: (Multimodal) Large Language Models\n\n\n\n\n\n\nLLM\n\n\nOverview\n\n\n\nThis is an overview of the Muultimodal Large Language Models. It includes the defintion of the large language models, the multimodal large language models, and the applications of those models. Besides, it also includes to fine-tune those models with different methods like LoRA.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n7 words\n\n\n10/25/24, 12:42:57 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Deep Meta Learning\n\n\n\n\n\n\nMeta Learning\n\n\nOverview\n\n\n\nThis is an overview of the Meta Learning. It includes the definition of the multi-task learning, transfer learning and meta learning. Three most common types of meta-learning algorithms, metric learning, model-based learning, and optimization-based learning are also included. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 12:42:54 AM\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Foundations and Concepts: Chapter 03 Summary and Exercise Solutions\n\n\n\n\n\n\nDLFC\n\n\n\nThis is my understanding of the paper Attention is all you need. This paper introduces the most important model in the Deep Learning field: Transformer. The Transformer is the corner stone of current Large Language Model. This blog includes theory and implmentation of the Transformer using Pytorch from scratch.\n\n\n\n\n\nOct 24, 2024\n\n\n1 min\n\n\n19 words\n\n\n10/25/24, 10:49:07 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Basic/MLE.html",
    "href": "posts/Basic/MLE.html",
    "title": "Maximum Likelihood Estimation: From Basic to Advance",
    "section": "",
    "text": "Maximum Likelihood Estimation is foundational across machine learning, helping estimation parameters by maximizing the likelihood of obseved data. It serves as the backbone for various algorithms, from classical linear regression to deep neural networks. This blog walks through MLE’s foundations, how it apply to the modern deep learning era and how to solve the MLE problem when it lack a closed-form solution.\n\nThe Idea of Maximum Likelihood Estimation\nThe goal of MLE is to find the parameters of a probabilistic model that maximize the likelihood of the observed data. Formally, for a dataset \\(\\mathcal{D} = \\{x_1, x_2, \\dots, x_n \\}\\), we assume each data point \\(x_i\\) is independently generated from a probability distribution \\(p(x | \\theta)\\), parameterized by \\(\\theta\\). The MLE seeks the parameters \\(\\theta\\) that maximize the likelihood of the observed data, which is the product of the probability of each data point: \\[\nL(\\theta; \\mathcal{D}) = \\prod_{i=1}^{n} p(x_i | \\theta)\n\\]\nIn practice, the logarithm of the likelihood function, known as the log-likelihood, is maximized for numerical stability: \\[\n\\log L(\\theta; \\mathcal{D}) = \\sum_{i=1}^{n} \\log p(x_i | \\theta)\n\\] We can use optimization techniques like Stochastic Gradient Descent (SGD) to maximize the log-likelihood when a closed-form solution is not available. Specifically, MLE aims to maximize the likelihhod of the observed data given the model parameters, which translates into minimizing the negative log-likelihood(NNL) as the loss function. And optimize the loss function using numerical optimization techniques like SGD. For example\nIn the contex of the multi-class classification problem, the likelihood for the entire dataset of zie \\(n\\) is given by: \\[\nL(\\theta; \\mathcal{D}) = \\prod_{i=1}^{n} p(y_i | x_i, \\theta)\n\\] where \\(y_i\\) is the label of the \\(i\\)-th data point \\(x_i\\). The log-likelihood is then: \\[\n\\log L(\\theta; \\mathcal{D}) = \\sum_{i=1}^{n} \\log p(y_i | x_i, \\theta)\n\\] The negative log-likelihood (NNL) is used as the loss function to be minimized: \\[\n\\text{NNL}(\\theta; \\mathcal{D}) = - \\sum_{i=1}^{n} \\log p(y_i | x_i, \\theta)\n\\] In the deep neural networks, the output layer for multi-class classfication is usually a softmax function. The softmax function converts raw output scores(logits) into probabilities for each class: \\[\np(y_i | x_i, \\theta) = \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\] where \\(f_j\\) is the raw output score for class \\(j\\), and \\(C\\) is the number of classes. The negative log-likelihood loss function for the softmax output is the cross-entropy loss: \\[\n\\text{NNL}(\\theta; \\mathcal{D}) = - \\sum_{i=1}^{n} \\log \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\] The cross-entropy loss for the entire dataset is the average of the loss for each data point: \\[\n\\mathcal{L}(\\theta; \\mathcal{D}) = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i, c}\\log \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\]\nWe can use SGD to minimize the cross-entropy loss by updating the model parameters \\(\\theta\\) in the opposite direction of the gradient of the loss function with respect to \\(\\theta\\).\n\n\nBasic MLE Example: Single Gaussian Density Estimation\nTo illustrate the concept of MLE, let’s consider a simple example of estimating the parameters of a single Gaussian distribution. Given a dataset \\(\\mathcal{D} = \\{x_1, x_2, \\dots, x_n \\}\\), the likelihood of the data under the Gaussian distribution is: \\[\nL(\\mu, \\sigma^2; \\mathcal{D}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\]"
  }
]