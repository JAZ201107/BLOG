[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a master student of Singapore Management University majoring in IT in Business with AI track. I am passionate about AI. I have a strong background in computer science and big data. I am proficient in Python. I have experience in machine learning, deep learning. I am a quick learner and a good team player. I like to learn new things summary and show them with each other. Currently I’m looking for a full-time job in AI field."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nSingapore Management University | Singapre Master of IT in Business(AI track) | Sept 2023 -\nUniversity of Wollongong | Computer Science B.S in Big Data | March 2020 - March 2023"
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nMachine Learning & Deep Learning\nComputer Vision\nNatural Language Processing\nReinforcement Learning\nMeta Learning\nGenerative Model\nUnsupervised Learning\nLarge Language Model\nConvex Optmization\nProbabilitic Graphical Model\nDeep Graph Learning"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About Me",
    "section": "Projects",
    "text": "Projects"
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html",
    "href": "posts/Overviews/Overview_PGM.html",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "",
    "text": "Probabilistic Graphical Models(PGMs) are a crucial tool for reasoning under uncertainty, merging probability theory with graph theory to represent and analyze complex relationships between variables. PGMs help address problems that require modeling uncertainty in areas like machine learning, artificial iintelligence and so on. They also provide a structured framework for deciion-makeing, making them highly relevant to fields such as deep learning and reinforcement learning.\nFor example, PGMs play a foundational role by inspiring architectures that manage uncertainty, such as Bayesian Neural Networks and vairation inference techniques. They allow neural networks to quantify uncertainty, enhancing robustness in fields like medical diagnosis and autonomous driving. In reinforcement leaning, PGM facilitate modeling environments with partially observable Markov Decision processes, where agents make decisions under uncertainty. PGMs help RL algorithms in state eatimation and policy learning by providing a framework to manage incomplelte information about the environment.\nIn this blog, we will go through some essential aspects of PGMs, including representation, inference, and learning methods. We will discuss the two main types of PGMs, Bayesian Networks and Markov Networks, and Factor Graphs which unify both. Next we will explore inference algorithms from two asepect: exact inference methods like Variable Elimination and approximate inference methods like Monte Carlo methods and Variational Inference. Finally, we will discuss learning methods like Maximum Likelihood Estimation, Maximum A Posteriori Estimation, Expectation Maximization and structure learning when we do not have the structure of the model. In the end, we will discuss some applications of PGMs and how to combine with deep nerual networks to build more robust models."
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#bayesian-networks",
    "href": "posts/Overviews/Overview_PGM.html#bayesian-networks",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Bayesian Networks",
    "text": "Bayesian Networks\nBayesian Networks also known as Belief Networks or Directed Acyclic Graphs(DAGs), are a type of PGM that represent the conditional dependencies between random variables. The nodes in a BN represent random variables, and the edges represent the direct dependencies between them. The conditional dependencies are encoded using the conditional probability distribution of each node given its parents.\nLet us consider a simple example of a student model, where the grade of a student depends on the difficulty of the course, the student’s intelligence, and the SAT score. The Bayesian Network for this model is shown below:\n\n\n\n\n\n\n\n\nG\n\n\n\nDifficulty\n\nDifficulty\n\n\n\nGrade\n\nGrade\n\n\n\nDifficulty-&gt;Grade\n\n\n\n\n\nLetter\n\nLetter\n\n\n\nGrade-&gt;Letter\n\n\n\n\n\nIntelligence\n\nIntelligence\n\n\n\nIntelligence-&gt;Grade\n\n\n\n\n\nSAT\n\nSAT\n\n\n\nIntelligence-&gt;SAT\n\n\n\n\n\n\n\n\nFigure 1: Student Model\n\n\n\n\n\nIn the Bayesian Network, the joint distribution can be factorized into the product of the conditional probabilities of each node given its parents. This factorization allows for efficient inference and learning in the model. The above Bayesian Network represents the following conditional dependencies: \\[\nP(D, I, S, G, L) = P(D)P(I)P(S|I)P(G|D, I)P(L|G)\n\\]\nSo, the joint probability can be factorized into conditional probabilties of each vairable given its parents. \\[\nP(X_1, X_2, \\dots, X_n) = \\prod_{i=1}^{n} P(X_i | \\text{Pa}(X_i))\n\\] where \\(\\text{Pa}(X_i)\\) represents the parents of node \\(X_i\\) in the Bayesian Network.\nOn the other hand, we can view the graph as encoding a generative sampling process executed in a topological order, starting from the root nodes and samples the child nodes given their parents. For example, in the student model, we can sample the grade of the student by first sampling the difficulty, intelligence, and SAT score, and then the grade and letter.\nThere is probabilic distirbution associalte with each node in the Bayesian Network called Conditional Probability Distribution(CPD). The CPD of a node \\(X_i\\) is the probability distribution of \\(X_i\\) given its parents \\(\\text{Pa}(X_i)\\). For example, the CPD of the grade node in the student model is: \\[\nP(G=A | D, I) = \\begin{cases}\n0.3 & \\text{if } D = \\text{Easy}, I = \\text{Low} \\\\\n0.7 & \\text{if } D = \\text{Easy}, I = \\text{High} \\\\\n0.9 & \\text{if } D = \\text{Hard}, I = \\text{Low} \\\\\n0.1 & \\text{if } D = \\text{Hard}, I = \\text{High} \\\\\n\\end{cases}\n\\]\nWhen we are given a Bayesian Network, we want to perform inference to answer queries about the model. For example, how the difficulty of the course change when we know the grade of the student.\n\n\n\n\n\n\nInference in Bayesian Networks\n\n\n\n\n\nInference in Bayesian Networks is the process of answering queries about the model, such as computing the posterior distribution of a variable given evidence.\n\n\n\nThe representation of the Bayesian Network of the above inference problem is shown below:\n\n\n\n\n\n\n\n\nG\n\n\n\nDifficulty\n\nDifficulty\n\n\n\nGrade\n\nGrade\n\n\n\nDifficulty-&gt;Grade\n\n\n\n\n\nLetter\n\nLetter\n\n\n\nGrade-&gt;Letter\n\n\n\n\n\nIntelligence\n\nIntelligence\n\n\n\nIntelligence-&gt;Grade\n\n\n\n\n\nSAT\n\nSAT\n\n\n\nIntelligence-&gt;SAT\n\n\n\n\n\n\n\n\nFigure 2: Inference in Bayesian Networks The goal is to compute the posterior distribution of the difficulty given the grade.\n\n\n\n\n\nWe use shadowed nodes to represent the observed variables, and the goal is to compute the posterior distribution of the difficulty \\(P(D|G)\\) given the grade.\nThere are two main inference algorithms for Bayesian Networks: * Exact Inference: Variable Elimination, Belief Propagation * Approximate Inference: Gibbs Sampling, Variational Inference We leave the details of the inference in the Bayesian Networks for the next section.\nThe next question raise is the when we are given are dataset, how can we learn the structure and parameters of the Bayesian Network. This is where the learning methods come into play.\nNext Let introduce I-Maps. I-Maps are the independencies that are implied by the graph structure of the Bayesian Network. It establish the relationship between graph and distribution. A distribution The I-Maps are used to perform efficient inference in the model by exploiting the conditional independence relationships between the variables. For example, in the student model, the grade is independent of the SAT score given the intelligence of the student. This independence relationship can be used to simplify the computation of the posterior distribution of the grade given the SAT score and intelligence. Formally, a graph \\(G\\) is called an I-map of probability distribution \\(P\\) if all conditional indeoendence relationships implied by the graph \\(G\\) aslo hold in teh distribution \\(P\\). This means that: \\[\nI(G) \\subseteq I(P)\n\\]\nOn the definition we can see that one probability distribution can have multiple I-Maps. The I-Maps are used to perform efficient inference in the model by exploiting the conditional independence relationships between the variables. For example, in the student model, the grade is independent of the SAT score given the intelligence of the student. This independence relationship can be used to simplify the computation of the posterior distribution of the grade given the SAT score and intelligence.\nOne Important thing about I-map is that it can not miss any independencies that are present in the distribution. If the graph \\(G\\) is an I-map of the distribution \\(P\\), then the graph \\(G\\) is called a perfect map of the distribution \\(P\\). This means that the graph \\(G\\) captures all the independencies in the distribution \\(P\\) and can be used for efficient inference and learning in the model.\nWhen we have a Bayesian Network, we determine whether two variables in the network are conditionally independent through D-separation. D-separation is a graphical criterion that determines whether two variables are conditionally independent given a set of observed variables. The D-separation criterion is based on the concept of active trails in the graph, which are paths between two variables that are not blocked by any observed variables. If there is an active trail between two variables, they are not conditionally independent given the observed variables. If all trails between two variables are blocked by the observed variables, they are conditionally independent. The D-separation criterion is used to determine the conditional independence relationships in the Bayesian Network and establish the I-maps of the model.\nD-Separation is used as a tool to determin whether a given graph can serve an I-map for a given distribution.\n\nApplications of Bayesian Networks\nBayesian Networks are widely used in various applications, such as: * Medical Diagnosis: Modeling the dependencies between symptoms and diseases. * Anomaly Detection: Detecting unusual patterns in network traffic or financial transactions. * Robotics: Modeling the environment for robot localization and mapping. * Natural Language Processing: Modeling the dependencies between words in a sentence."
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#markov-networks",
    "href": "posts/Overviews/Overview_PGM.html#markov-networks",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Markov Networks",
    "text": "Markov Networks\nMarkov Networks also named as Undirted Graphical Models.\nMarkov Networks are undirected graphs that represent the joint distribution of random variables. The nodes in a Markov Network represent random variables, and the edges represent the interactions between them. Unlike Bayesian Networks, Markov Networks do not have directed edges, and the interactions are symmetric. The joint distribution of the random variables is factorized into potential functions that capture the interactions between the variables.\nUnlike parent-child relationships in Markov network, we cannot assign a conditional probabilituy table to each node and apply the chain rule to compute the joint distribution as we do in Bayesian Networks. Instead, we assign a factor to each maximal clique in the graph, which is a subset of nodes that are fully connected. The joint distribution is then factorized into the product of potential functions over the cliques in the graph: \\[\nP(X_1, X_2, \\dots, X_n) = \\frac{1}{Z} \\prod_{C \\in \\mathcal{C}} \\phi_C(X_C)\n\\] where \\(\\mathcal{C}\\) is the set of maximal cliques in the graph, \\(\\phi_C(X_C)\\) is the potential function over the clique \\(C\\), and \\(Z\\) is the normalization constant called the partition function, defined as: \\[\nZ = \\sum_{X_1, X_2, \\dots, X_n} \\prod_{C \\in \\mathcal{C}} \\phi_C(X_C)\n\\]\nFor example:\n\nThe Markov Field is defiend as the Gibbs Distribution(Blozmann Distribution) over the graph. The Gibbs Distribution is defined as: \\[\nP(X) = \\frac{1}{Z} \\exp\\left(-\\sum_{C \\in \\mathcal{C}} \\phi_C(X_C)\\right)\n\\] where \\(Z\\) is the partition function, and \\(\\phi_C(X_C)\\) is the potential function over the clique \\(C\\).\nThe Hammersley-Clifford Theorem states that any positive distribution over a set of random variables can be represented as a Markov Network. This theorem provides a theoretical foundation for the use of Markov Networks to model complex dependencies between random variables.\nTo improve he Markove Network, notes that the distribution can not be nagetive, so we can not use the general Gibbs Distributions, because the potential function can be nagetive. To solve this problem, we can use the Exponential Form of the Gibbs distributions, which is always non-negative. We represent a clique potential \\(\\psi(x_c)\\) in an unconstrained form value “energy” function \\(\\psi(x_c)=\\exp(-\\phi_c(x_c))\\). The joint probability has a nice additive form: \\[\nP(X) = \\frac{1}{Z} \\exp\\left(-\\sum_{C \\in \\mathcal{C}} \\phi_C(X_C)\\right) = \\frac{1}{Z} \\exp\\left(-H(x)\\right)\n\\] where \\(H(x)\\) is the free energy function of the Markov Network. This is called the Bolztmann Distribution(also called log-linear model in statistics)."
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#factor-graph",
    "href": "posts/Overviews/Overview_PGM.html#factor-graph",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Factor Graph",
    "text": "Factor Graph\nFactor Graphs are a generalization of Bayesian Networks and Markov Networks that unify both representations. Factor Graphs represent the joint distribution of random variables using factors that capture the dependencies between the variables. The nodes in a Factor Graph represent random variables, and the edges represent the factors that connect the variables. Factor Graphs provide a flexible framework for modeling complex dependencies between random variables and can be used to represent both Bayesian Networks and Markov Networks. For example, the student model can be represented as a Factor Graph as shown below:\n\n\n\n\n\n\n\n\nG\n\n\n\nDifficulty\n\nDifficulty\n\n\n\nGrade\n\nGrade\n\n\n\nDifficulty--Grade\n\n\n\n\nLetter\n\nLetter\n\n\n\nGrade--Letter\n\n\n\n\nIntelligence\n\nIntelligence\n\n\n\nIntelligence--Grade\n\n\n\n\nSAT\n\nSAT\n\n\n\nIntelligence--SAT\n\n\n\n\n\n\n\nFigure 3: Factor Graph of Student Model\n\n\n\n\n\nIn the Factor Graph, the nodes represent random variables, and the edges represent the factors that connect the variables. The joint distribution of the random variables is factorized into the product of factors over the cliques in the graph: \\[\nP(X_1, X_2, \\dots, X_n) = \\frac{1}{Z} \\prod_{f \\in \\mathcal{F}} \\phi_f(X_f)\n\\] where \\(\\mathcal{F}\\) is the set of factors in the graph, \\(\\phi_f(X_f)\\) is the factor over the variables \\(X_f\\), and \\(Z\\) is the normalization constant called the partition function.\nFactor Graphs provide a unified representation of Bayesian Networks and Markov Networks and can be used to model complex dependencies between random variables. They are widely used in various applications, such as: * Error-Correcting Codes: Modeling the dependencies between bits in a code. * Sensor Networks: Modeling the dependencies between sensors in a network. * Image Processing: Modeling the dependencies between pixels in an image. * Social Networks: Modeling the dependencies between users in a network. * Robotics: Modeling the dependencies between sensors and actuators in a robot. * Natural Language Processing: Modeling the dependencies between words in a sentence."
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#exact-inference",
    "href": "posts/Overviews/Overview_PGM.html#exact-inference",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Exact Inference",
    "text": "Exact Inference\n\nThe Elimination Algorithms\n\nVariable Elimination\n\n\nBucket Elimination\n\n\n\nMessage Passing Algorithms\n\nBelief Propagation\n\n\nSum Product Algorithm\n\n\n\nThe Junction Tree Algorithm"
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#approximate-inference",
    "href": "posts/Overviews/Overview_PGM.html#approximate-inference",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Approximate Inference",
    "text": "Approximate Inference\n\nMonte Carlo Methods\n\nMarkov Chain Monte Carlo (MCMC)\n\n\n\nVariational Algorithms"
  },
  {
    "objectID": "posts/Overviews/Overview_PGM.html#parameter-learning",
    "href": "posts/Overviews/Overview_PGM.html#parameter-learning",
    "title": "Overview: Probabilistic Graphical Models",
    "section": "Parameter Learning",
    "text": "Parameter Learning\n\nMaximum Likelihood Estimation\n\\[\n\\theta = \\underset{\\theta}{\\operatorname{arg\\max}\\ } P(\\mathcal{D} | \\theta)\n\\]\nFor more details check out this blog Maximum Likelihood Learning. ### Maximum A Posteriori Estimation\n\\[\n\\theta = \\underset{\\theta}{\\operatorname{arg\\max}} \\overbrace{ P(\\mathcal{D} |\\theta) }^{ \\text{Likelihood} }\\underbrace{ P(\\theta) }_{ \\text{Prior Distribution} }\n\\]\n\n\nExpectation Maximization\nThis is a general framework for learning in probabilistic models with latent variables. The EM algorithm alternates between the E-step, where the posterior distribution of the latent variables is computed, and the M-step, where the model parameters are updated to maximize the expected log-likelihood. The EM algorithm is used in models like Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) to estimate the parameters of the model when the latent variables are not observed.\nFor More Details, check out this blogs Expectation Maximization ## Structure Learning ### Chow-Liu Algorithm"
  },
  {
    "objectID": "posts/Overviews/Overview_MetaLearning.html",
    "href": "posts/Overviews/Overview_MetaLearning.html",
    "title": "Overview: Deep Meta Learning",
    "section": "",
    "text": "Overview of the Deep Meta Learning"
  },
  {
    "objectID": "posts/Overviews/Overview_Optimization.html",
    "href": "posts/Overviews/Overview_Optimization.html",
    "title": "Overview: Optimization Algorithms",
    "section": "",
    "text": "Overview of Optimization Algorithms"
  },
  {
    "objectID": "posts/Readings/DLFC/Chapter02.html",
    "href": "posts/Readings/DLFC/Chapter02.html",
    "title": "Deep Learning Foundations and Concepts: Chapter 02 Summary and Exercise Solutions",
    "section": "",
    "text": "Deep Learning Foundations and Concepts Chapter 02 Summary and Exercise Solutions"
  },
  {
    "objectID": "posts/Basic/MDP.html",
    "href": "posts/Basic/MDP.html",
    "title": "From Markov Chain to Markov Decision Process and MCMC",
    "section": "",
    "text": "Markov Chain\n\n\nMarkov Decision Process\n\n\nMarkov Chain Monte Carlo"
  },
  {
    "objectID": "posts/Basic/EM.html",
    "href": "posts/Basic/EM.html",
    "title": "Expectation Maximization",
    "section": "",
    "text": "From K-Means Algorithms to EM\n\n\nFrom Gaussian Mixture Model\n\n\nGeneral Expectation Maximization Algorithms\n\n\nEvidence Lower Bound\n\n\nCombined With Deep Neural Network\n\n\nConclusion"
  },
  {
    "objectID": "posts/Papers/Attention_is_all_you_need.html",
    "href": "posts/Papers/Attention_is_all_you_need.html",
    "title": "Dive into: Attention is all you need",
    "section": "",
    "text": "Paper Reading: Attention is all you need\n\n\nReference"
  },
  {
    "objectID": "posts/Basic/MLE.html",
    "href": "posts/Basic/MLE.html",
    "title": "Maximum Likelihood Learning: From Basic to Advance",
    "section": "",
    "text": "Maximum Likelihood Estimation is foundational across machine learning, helping estimation parameters by maximizing the likelihood of observed data. It serves as the backbone for various algorithms, from classical linear regression to deep neural networks. This blog walks through MLE’s foundations, how it apply to the modern deep learning era and how to solve the MLE problem when it lack a closed-form solution.\n\nThe Idea of Maximum Likelihood Estimation\nThe goal of MLE is to find the parameters of a probabilistic model that maximize the likelihood of the observed data. Formally, for a dataset \\(\\mathcal{D} = \\{x_1, x_2, \\dots, x_n \\}\\), we assume each data point \\(x_i\\) is independently generated from a probability distribution \\(p(x | \\theta)\\), parameterized by \\(\\theta\\). The MLE seeks the parameters \\(\\theta\\) that maximize the likelihood of the observed data, which is the product of the probability of each data point: \\[\nL(\\theta; \\mathcal{D}) = \\prod_{i=1}^{n} p(x_i | \\theta)\n\\]\nIn practice, the logarithm of the likelihood function, known as the log-likelihood, is maximized for numerical stability: \\[\n\\log L(\\theta; \\mathcal{D}) = \\sum_{i=1}^{n} \\log p(x_i | \\theta)\n\\] We can use optimization techniques like Stochastic Gradient Descent (SGD) to maximize the log-likelihood when a closed-form solution is not available. Specifically, MLE aims to maximize the likelihhod of the observed data given the model parameters, which translates into minimizing the negative log-likelihood(NNL) as the loss function. And optimize the loss function using numerical optimization techniques like SGD. For example\nIn the context of the multi-class classification problem, the likelihood for the entire dataset of zie \\(n\\) is given by: \\[\nL(\\theta; \\mathcal{D}) = \\prod_{i=1}^{n} p(y_i | x_i, \\theta)\n\\] where \\(y_i\\) is the label of the \\(i\\)-th data point \\(x_i\\). The log-likelihood is then: \\[\n\\log L(\\theta; \\mathcal{D}) = \\sum_{i=1}^{n} \\log p(y_i | x_i, \\theta)\n\\] The negative log-likelihood (NNL) is used as the loss function to be minimized: \\[\n\\text{NNL}(\\theta; \\mathcal{D}) = - \\sum_{i=1}^{n} \\log p(y_i | x_i, \\theta)\n\\] In the deep neural networks, the output layer for multi-class classfication is usually a softmax function. The softmax function converts raw output scores(logits) into probabilities for each class: \\[\np(y_i | x_i, \\theta) = \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\] where \\(f_j\\) is the raw output score for class \\(j\\), and \\(C\\) is the number of classes. The negative log-likelihood loss function for the softmax output is the cross-entropy loss: \\[\n\\text{NNL}(\\theta; \\mathcal{D}) = - \\sum_{i=1}^{n} \\log \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\] The cross-entropy loss for the entire dataset is the average of the loss for each data point: \\[\n\\mathcal{L}(\\theta; \\mathcal{D}) = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i, c}\\log \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\]\nWe can use SGD to minimize the cross-entropy loss by updating the model parameters \\(\\theta\\) in the opposite direction of the gradient of the loss function with respect to \\(\\theta\\).\n\n\nBasic MLE Example: Single Gaussian Density Estimation\nTo illustrate the concept of MLE, let’s consider a simple example of estimating the parameters of a single Gaussian distribution. Given a dataset \\(\\mathcal{D} = \\{x_1, x_2, \\dots, x_n \\}\\), the likelihood of the data under the Gaussian distribution is: \\[\nL(\\mu, \\sigma^2; \\mathcal{D}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\]\n\n\nIntermediate MLE Example: Linear Regression\nNext we go through a more complex example of MLE in the context of linear regression. Given a dataset \\(\\mathcal{D} = \\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) \\}\\), the linear regression model assumes the relationship between the input \\(x\\) and the output \\(y\\) is linear: \\[\ny = w x + b + \\epsilon\n\\] where \\(\\epsilon\\) is the noise term. The likelihood of the data under the linear regression model is:\nThe Likelihood of the data under the linear regression model is: \\[\nL(w, b; \\mathcal{D}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - w x_i - b)^2}{2\\sigma^2}\\right)\n\\] where \\(\\sigma^2\\) is the variance of the noise term \\(\\epsilon\\). The log-likelihood is: \\[\n\\log L(w, b; \\mathcal{D}) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - w x_i - b)^2\n\\]\nThe first term in the log-likelihood is a constant, so maximizing the log-likelihood is equivalent to minimizing the sum of squared errors between the predicted output \\(w x_i + b\\) and the true output \\(y_i\\). This is the same as the objective function in the ordinary least squares (OLS) method for linear regression. \\[\n\\text{OLS Loss} = \\sum_{i=1}^{n} (y_i - w x_i - b)^2\n\\]\n\n\nAdvanced MLE Example: Multi-Class Classification with Deep Learning\nFinally, we explore MLE in the context of multi-class classification using deep learning. Given a dataset \\(\\mathcal{D} = \\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) \\}\\), the deep neural network model predicts the probability distribution over the classes for each input \\(x_i\\): \\[\np(y_i | x_i, \\theta) = \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\] where \\(f_j\\) is the raw output score for class \\(j\\), and \\(C\\) is the number of classes. The negative log-likelihood loss function for the softmax output is the cross-entropy loss: \\[\n\\text{NNL}(\\theta; \\mathcal{D}) = - \\sum_{i=1}^{n} \\log \\frac{e^{f_{y_i}}}{\\sum_{j}^{C} e^{f_j}}\n\\]\nWe can use SGD to minimize the cross-entropy loss by updating the model parameters \\(\\theta\\) in the opposite direction of the gradient of the loss function with respect to \\(\\theta\\).\n\n\nAdvanced MLE Exmples: MLE in Generative Model\nMLE is also used in generative models like Gaussian Mixture Models (GMM) and Variational Autoencoders (VAEs). In GMM, the likelihood of the data under the model is a mixture of Gaussian distributions. The MLE estimates the parameters of the Gaussian components to maximize the likelihood of the observed data. In VAEs, the MLE is used to train the encoder and decoder networks to maximize the likelihood of the data under the generative model.\nIn the generative model case, we want to learn a probability distribution \\(P(x)\\) over image to do: * Generation: If we sample \\(x_{\\text{new}} \\sim P(X)\\), \\(x_{\\text{new}}\\) should look like the real image (sampling) * Density Estimation: \\(P(x)\\) should be high if \\(x\\) looks like a real image (Anomaly Detecion) * Unsupervised Representation: We should be able to learn what these images have in common. But the question raise: 1. How to represent \\(P_{\\theta}(x)\\): the \\(x\\) is high-dimentional, Normal Distribution cannot be used here 2. How to learn the parameter of the \\(P_{\\theta}(x)\\)\nThe goal of learning is to return a model \\(P_{\\theta}(x)\\) that precisely captures the distributions \\(P_{\\text{data}}(x)\\) from which out data was sampled.\nHere we use KL-divergence to measure how two distribution are close to each other. \\[\n\\begin{split}\nD(p \\| q) & =  \\sum_{x}p(x)\\log \\frac{p(x)}{q(x)} \\\\\n& = \\mathbb{E}_{x \\sim p_{data}}\\left[ \\log \\left( \\frac{P_{\\text{data}}(x)}{P_{\\theta}(x)} \\right) \\right] \\\\\n& = \\mathbb{E}_{x \\sim p_{data}}[\\log P_{\\text{data}}(x)] - \\mathbb{E}_{x \\sim p_{data}}[\\log P_{\\theta}(x)]\n\\end{split}\n\\] Since the first term is the does not depend on \\(P_{\\theta}\\), so, we just need the maximize the second term to minimize the KL-divergence. That mean we need to maximizing the expected log-likelihood: \\[\n\\underset{P_{\\theta}}{\\operatorname{arg\\min}\\ } D(P_{data} \\| P_{\\theta}) = \\underset{P_{\\theta}}{\\operatorname{arg\\max}\\ } \\mathbb{E}_{x \\sim p_{data}}[\\log P_{\\theta}(x)]\n\\]\nSince we can not get the exact value of the \\(\\mathbb{E}_{x \\sim p_{data}}[\\log P_{\\theta}(x)]\\), we can use empirical log-likelihood: \\[\n\\mathbb{E}_{x \\sim p_{data}}[\\log P_{\\theta}(x)] \\approx \\frac{1}{|\\mathcal{D}|}\\sum_{x \\in \\mathcal{D}}\\log P_{\\theta}(x)\n\\]\n\n\nChallenges and Limitations of MLE\nFinally, we disscuss some of the challenges and limitations of MLE. One of the main challenges of MLE is that it requires a closed-form solution for the likelihood function, which is not always available, especially in complex models like deep neural networks. In such cases, numerical optimization techniques like SGD are used to maximize the log-likelihood. Another limitation of MLE is that it assumes the data is generated from a specific parametric model, which may not always be the case in practice. In such cases, more flexible models like non-parametric methods or Bayesian approaches may be more appropriate. There are some other limitations of MLE, such as overfitting, local optima, and sensitivity to outliers, which can be addressed by regularization techniques, initialization strategies, and robust loss functions.\n\n\nAlternative to MLE\nThere are some methods when the MLE is not good for the problem. Maximum Posterior Estimation (MAP) is one of the alternatives to MLE. MAP incorporates prior knowledge about the parameters into the estimation process by maximizing the posterior distribution of the parameters given the data. Bayesian methods are another alternative to MLE, which treat the parameters as random variables and estimate their posterior distribution using Bayes’ theorem. Bayesian methods provide a more principled way to incorporate prior knowledge and uncertainty into the estimation process.\n\n\nConclusion\nIn conclusion, Maximum Likelihood Estimation (MLE) is a fundamental concept in machine learning that aims to estimate the parameters of a probabilistic model by maximizing the likelihood of the observed data. MLE is used across a wide range of machine learning algorithms, from basic density estimation to advanced deep learning models. When a closed-form solution is not available, numerical optimization techniques like SGD can be used to maximize the log-likelihood. MLE has its challenges and limitations, such as the need for a closed-form solution and the assumption of a specific parametric model. However, MLE remains a powerful tool for parameter estimation in machine learning and provides a solid foundation for more advanced methods like Bayesian inference."
  },
  {
    "objectID": "posts/Readings/DLFC/Chapter03.html",
    "href": "posts/Readings/DLFC/Chapter03.html",
    "title": "Deep Learning Foundations and Concepts: Chapter 03 Summary and Exercise Solutions",
    "section": "",
    "text": "This is the content of what I learning in the DLFC.\nHighlights of thehioho\ndas a a da"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html",
    "href": "posts/Overviews/Overview_DeepRL.html",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "",
    "text": "In recent years, Deep Reinforcement Learning has emerged as one of the most promising areas within the broader field of Artificial Intelligence(AI). It combines two powerful tools: reinforcement learning(RL), where agents learn by interacting with their environment, and deep learning, where neural networks identify patterns from large amounts of data.\nThe Markov Decision Process provide the mathematical foundation for DRL, enabling agents to make intelligent decision in complex environment. Let defines some key concepts first, for more details of the MDP, please check the following blog"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#classical-deepq-learning",
    "href": "posts/Overviews/Overview_DeepRL.html#classical-deepq-learning",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Classical (Deep)Q-Learning",
    "text": "Classical (Deep)Q-Learning\nThe classic Q-learning finds the optimal state-action value function \\(Q(s, a)\\) through the Bellman Equation. The Q-value update rule is: \\[\nQ(s_{t}, a_{t}) \\leftarrow  Q(s_{t}, a_{t}) + \\alpha(R_{t + 1} + \\gamma \\underset{a'}{\\max \\ }Q(s_{t+1}, a') - Q(s_{t}, a_{t}))\n\\] When we use the neural network as function approximator, we need define an loss function to update the parameters. The Loss function is: \\[\n\\mathcal{L}(\\theta) = \\mathbb{E}[(R_{t+1} + \\gamma \\underset{a'}{\\max \\ }Q(s_{t+1}, a'; \\theta^{-}) - Q(s_{t}, a_{t}; \\theta) )^{2}]\n\\]\nThe problem of classical Q-Learning is that it suffer from overestimation bias, where the max operator selects actions that appear optimal due to noisy or high Q-value. To release this problem, we can use Double Q-Learning, it defined as"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#double-deep-q-learning",
    "href": "posts/Overviews/Overview_DeepRL.html#double-deep-q-learning",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Double (Deep) Q-Learning",
    "text": "Double (Deep) Q-Learning\nDouble Q-Learning reduces overestimation bias by maintaining two Q-tables(or two networks in deep learning settings). Each table is updated independently to decouple the action selection and value estimation processes. The update rule is: \\[\nQ_{1}(s_{t}, a_{t}) \\leftarrow Q_{1}(s_{t}, a_{t}) + \\alpha(R_{t+1} + \\gamma Q_{2}(s_{t + 1}, \\underset{a'}{\\operatorname{arg\\max}\\ }Q_{1}(s_{t+1}, a') ) - Q_{1}(s_{t}, a_{t}))\n\\]\nThe Loss Function for the Double Deep Q_Learning is that: \\[\n\\mathcal{L}(\\theta)=\\mathbb{E}[(R_{t+1} + \\gamma Q(s_{t+1}, \\underset{a'}{\\operatorname{arg\\max}\\ }Q(s_{t+1}, a'; \\theta);\\theta^{-} )-Q(s_{t}, a_{t}; \\theta))^{2}]\n\\]"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#dueling-dqn",
    "href": "posts/Overviews/Overview_DeepRL.html#dueling-dqn",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Dueling DQN",
    "text": "Dueling DQN\nThe problem of the Basic DQN is that not all actions are equally relevant in every state, but DQN treats all state-action equally, to solve this problem, we can use Dueling DQN, which separates the Q-value into two components: * State-Value function \\(V(s)\\): measures the value of being in state \\(s\\) * Advantage function \\(A(s, a)\\), which measures the relative importance of taking action \\(a\\) in state \\(s\\).\nThe Q-value is them computed as: \\[\nQ(s, a) = V(s) + A(s, a) - \\underset{a'}{\\max \\ } A(s, a')\n\\]"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#noisy-q-learning",
    "href": "posts/Overviews/Overview_DeepRL.html#noisy-q-learning",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Noisy Q-Learning",
    "text": "Noisy Q-Learning\nIn som environments, the agents needs better exploration strategies to avoid getting suck in local optima. The Noisy-Q-Learning introduces parametric noise into the neural network’s parameters, allowing the agent to explore more efficitively"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#soft-q-learning",
    "href": "posts/Overviews/Overview_DeepRL.html#soft-q-learning",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Soft Q-Learning",
    "text": "Soft Q-Learning\nSoft Q-learning introduces an entropy term into the objective to encourage exploration. The agent seeks to maximize both the cumulative reawrd and the policy entropy, learning yo better exploration in uncertain environemnts\nThe Objective Function become: \\[\nJ(\\pi) =\\mathbb{E}\\left[ \\sum_{t}(R_{t} + \\alpha \\mathcal{H}(\\pi(\\cdot | s_{t}))) \\right]\n\\]"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#experience-replay-buffer",
    "href": "posts/Overviews/Overview_DeepRL.html#experience-replay-buffer",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Experience Replay Buffer",
    "text": "Experience Replay Buffer"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#reinforce",
    "href": "posts/Overviews/Overview_DeepRL.html#reinforce",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "REINFORCE",
    "text": "REINFORCE\nREINFORCE is the most basic algorithm used in the reinforcement learning algorithms."
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#actor-critic-methods",
    "href": "posts/Overviews/Overview_DeepRL.html#actor-critic-methods",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Actor-Critic Methods",
    "text": "Actor-Critic Methods\n\nBasic Actor-Critic Algorithm\n\n\nAdvantage Actor-Critic Algorithm\n\n\nAsyn Advantage Actor-Critic Algorithm (A3D)"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#trust-region-policy-optimization",
    "href": "posts/Overviews/Overview_DeepRL.html#trust-region-policy-optimization",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Trust Region Policy Optimization",
    "text": "Trust Region Policy Optimization"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepRL.html#proximal-policy-optimization",
    "href": "posts/Overviews/Overview_DeepRL.html#proximal-policy-optimization",
    "title": "Overview: Deep Reinforcement Learning",
    "section": "Proximal Policy Optimization",
    "text": "Proximal Policy Optimization\nThis is an simpller alternative to TRPO"
  },
  {
    "objectID": "posts/Overviews/Overview_DeepGenerativeModel.html",
    "href": "posts/Overviews/Overview_DeepGenerativeModel.html",
    "title": "Overview: Deep Generative Models",
    "section": "",
    "text": "Overview of the Deep Generative Models"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuyang’s Blog",
    "section": "",
    "text": "From Markov Chain to Markov Decision Process and MCMC\n\n\n\n\n\n\nBasic Techniques\n\n\nReinforcement Learning\n\n\n\nThis blog explores the Markov Chain, Markov Decision Process, and Markov Chain Monte Carlo(MCMC) algorithm. It covers the definition of Markov Chain, the mathematical foundation of Markov Decision Process, and the application of MCMC in Bayesian Inference.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n10 words\n\n\n10/28/24, 12:43:18 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Deep Reinforcement Learning\n\n\n\n\n\n\nReinforcement Learning\n\n\nOverview\n\n\n\nThis is an overview of the Deep Reinforcement Learning. It includes the defintion of the deep reinforcement learning, some popular algorithms for example deep Q learning and variants, policy gradient methods and actor-critic methods for example A3C, DDPG, and PPO. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n4 min\n\n\n726 words\n\n\n10/28/24, 12:41:55 AM\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Learning: From Basic to Advance\n\n\n\n\n\n\nBasic Techniques\n\n\nLearning\n\n\n\nThis blog explores Maximum Likelihhood Estimation(MLE) and its applications across machine learning, from basic densitty estimation to deep learning. It also discuess optmization techniques like SGD, when MLE lacks a closed-from soluition, which is most common in deep learning.\n\n\n\n\n\nOct 26, 2024\n\n\n8 min\n\n\n1,417 words\n\n\n10/27/24, 11:23:43 PM\n\n\n\n\n\n\n\n\n\n\n\n\nExpectation Maximization\n\n\n\n\n\n\nBasic Techniques\n\n\nLearning\n\n\n\nThis blog explores the Expectation Maximization(EM) algorithm, a powerful technique for estimating parameters in latent variable models. It covers the intuition behind EM, its applications in clustering and Gaussian Mixture Models(GMM), and how it overcomes the challenges of missing data.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n21 words\n\n\n10/27/24, 2:39:02 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Probabilistic Graphical Models\n\n\n\n\n\n\nProbability\n\n\nOverview\n\n\n\nThis is an overview of Probabilistic Graphical Models. It includes the definition of the Probabilistic Graphical Models, different types of PGMs like Bayesian Networks, Markov Networks, and the different inference algorithms like Variable Elimination, Gibbs Sampling, and Belief Propagation, Variance Inference. Also, it includes the learning methods like Maximum Likelihood Estimation, Maximum A Posteriori Estimation, and Expectation Maximization. \n\n\n\n\n\nOct 25, 2024\n\n\n12 min\n\n\n2,308 words\n\n\n10/27/24, 2:36:15 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: (Multimodal) Large Language Models\n\n\n\n\n\n\nLLM\n\n\nOverview\n\n\n\nThis is an overview of the Muultimodal Large Language Models. It includes the defintion of the large language models, the multimodal large language models, and the applications of those models. Besides, it also includes to fine-tune those models with different methods like LoRA.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/27/24, 2:25:01 AM\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Foundations and Concepts: Chapter 03 Summary and Exercise Solutions\n\n\n\n\n\n\nDLFC\n\n\n\nThis is my understanding of the paper Attention is all you need. This paper introduces the most important model in the Deep Learning field: Transformer. The Transformer is the corner stone of current Large Language Model. This blog includes theory and implmentation of the Transformer using Pytorch from scratch.\n\n\n\n\n\nOct 24, 2024\n\n\n1 min\n\n\n19 words\n\n\n10/25/24, 10:49:07 AM\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Foundations and Concepts: Chapter 02 Summary and Exercise Solutions\n\n\n\n\n\n\nDLFC\n\n\n\nThis is my understanding of the paper Attention is all you need. This paper introduces the most important model in the Deep Learning field: Transformer. The Transformer is the corner stone of current Large Language Model. This blog includes theory and implmentation of the Transformer using Pytorch from scratch.\n\n\n\n\n\nOct 28, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 2:58:33 AM\n\n\n\n\n\n\n\n\n\n\n\n\nDive into: Attention is all you need\n\n\n\n\n\n\nPaper\n\n\nCode Implementation\n\n\n\nThis is my understanding of the paper Attention is all you need. This paper introduces the most important model in the Deep Learning field: Transformer. The Transformer is the corner stone of current Large Language Model. This blog includes theory and implmentation of the Transformer using Pytorch from scratch.\n\n\n\n\n\nOct 28, 2024\n\n\n1 min\n\n\n3 words\n\n\n10/25/24, 2:56:35 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Optimization Algorithms\n\n\n\n\n\n\nOptmization\n\n\nOverview\n\n\n\nThis is an overview of the Optmization Algorithms. It includes the definition of the optimization algorithms, the most popular optimization algorithms like first-order optmization(e.g. SGD), higher-order optmization(e.g. Newton’s Method) algorithms. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 12:43:06 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Deep Generative Models\n\n\n\n\n\n\nGenerative Model\n\n\nOverview\n\n\n\nThis is an overview of the Deep Generative Models. It includes the most popular generative models like VAE, GAN, and Flow-based models. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 12:43:00 AM\n\n\n\n\n\n\n\n\n\n\n\n\nOverview: Deep Meta Learning\n\n\n\n\n\n\nMeta Learning\n\n\nOverview\n\n\n\nThis is an overview of the Meta Learning. It includes the definition of the multi-task learning, transfer learning and meta learning. Three most common types of meta-learning algorithms, metric learning, model-based learning, and optimization-based learning are also included. Besides, it also includes the applications of those models.\n\n\n\n\n\nOct 25, 2024\n\n\n1 min\n\n\n1 words\n\n\n10/25/24, 12:42:54 AM\n\n\n\n\n\n\nNo matching items"
  }
]