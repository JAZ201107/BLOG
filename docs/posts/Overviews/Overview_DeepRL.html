<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-10-25">
<meta name="description" content="This is an overview of the Deep Reinforcement Learning. It includes the defintion of the deep reinforcement learning, some popular algorithms for example deep Q learning and variants, policy gradient methods and actor-critic methods for example A3C, DDPG, and PPO. Besides, it also includes the applications of those models.">

<title>Overview: Deep Reinforcement Learning – Yuyang’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Overview: Deep Reinforcement Learning – Yuyang’s Blog">
<meta property="og:description" content="This is an overview of the Deep Reinforcement Learning. It includes the defintion of the deep reinforcement learning, some popular algorithms for example deep Q learning and variants, policy gradient methods and actor-critic methods for example A3C, DDPG, and PPO. Besides, it also includes the applications of those models.">
<meta property="og:image" content="Images/Overview_DRL.png">
<meta property="og:site_name" content="Yuyang's Blog">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Yuyang’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/JAZ201107"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/zhang-yuyang/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="1">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#key-concepts-and-notations" id="toc-key-concepts-and-notations" class="nav-link active" data-scroll-target="#key-concepts-and-notations">Key Concepts and Notations</a></li>
  <li><a href="#the-objective-function" id="toc-the-objective-function" class="nav-link" data-scroll-target="#the-objective-function">The Objective Function</a>
  <ul class="collapse">
  <li><a href="#algorithms" id="toc-algorithms" class="nav-link" data-scroll-target="#algorithms">Algorithms</a></li>
  </ul></li>
  <li><a href="#q-learning-and-variants" id="toc-q-learning-and-variants" class="nav-link" data-scroll-target="#q-learning-and-variants">Q-Learning and Variants</a>
  <ul class="collapse">
  <li><a href="#classical-deepq-learning" id="toc-classical-deepq-learning" class="nav-link" data-scroll-target="#classical-deepq-learning">Classical (Deep)Q-Learning</a></li>
  <li><a href="#double-deep-q-learning" id="toc-double-deep-q-learning" class="nav-link" data-scroll-target="#double-deep-q-learning">Double (Deep) Q-Learning</a></li>
  <li><a href="#dueling-dqn" id="toc-dueling-dqn" class="nav-link" data-scroll-target="#dueling-dqn">Dueling DQN</a></li>
  <li><a href="#noisy-q-learning" id="toc-noisy-q-learning" class="nav-link" data-scroll-target="#noisy-q-learning">Noisy Q-Learning</a></li>
  <li><a href="#soft-q-learning" id="toc-soft-q-learning" class="nav-link" data-scroll-target="#soft-q-learning">Soft Q-Learning</a></li>
  <li><a href="#experience-replay-buffer" id="toc-experience-replay-buffer" class="nav-link" data-scroll-target="#experience-replay-buffer">Experience Replay Buffer</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"></a></li>
  </ul></li>
  <li><a href="#policy-based-algorithm" id="toc-policy-based-algorithm" class="nav-link" data-scroll-target="#policy-based-algorithm">Policy Based Algorithm</a>
  <ul class="collapse">
  <li><a href="#reinforce" id="toc-reinforce" class="nav-link" data-scroll-target="#reinforce">REINFORCE</a>
  <ul class="collapse">
  <li><a href="#reinforce-with-baseline" id="toc-reinforce-with-baseline" class="nav-link" data-scroll-target="#reinforce-with-baseline">REINFORCE with Baseline</a></li>
  <li><a href="#off-policy-reinforce" id="toc-off-policy-reinforce" class="nav-link" data-scroll-target="#off-policy-reinforce">Off-Policy REINFORCE</a></li>
  <li><a href="#natural-gradient" id="toc-natural-gradient" class="nav-link" data-scroll-target="#natural-gradient">Natural Gradient</a></li>
  </ul></li>
  <li><a href="#actor-critic-methods" id="toc-actor-critic-methods" class="nav-link" data-scroll-target="#actor-critic-methods">Actor-Critic Methods</a>
  <ul class="collapse">
  <li><a href="#basic-actor-critic-algorithm" id="toc-basic-actor-critic-algorithm" class="nav-link" data-scroll-target="#basic-actor-critic-algorithm">Basic Actor-Critic Algorithm</a></li>
  <li><a href="#advantage-actor-critic-algorithm" id="toc-advantage-actor-critic-algorithm" class="nav-link" data-scroll-target="#advantage-actor-critic-algorithm">Advantage Actor-Critic Algorithm</a></li>
  <li><a href="#asyn-advantage-actor-critic-algorithm-a3c" id="toc-asyn-advantage-actor-critic-algorithm-a3c" class="nav-link" data-scroll-target="#asyn-advantage-actor-critic-algorithm-a3c">Asyn Advantage Actor-Critic Algorithm (A3C)</a></li>
  </ul></li>
  <li><a href="#trust-region-policy-optimization" id="toc-trust-region-policy-optimization" class="nav-link" data-scroll-target="#trust-region-policy-optimization">Trust Region Policy Optimization</a></li>
  <li><a href="#proximal-policy-optimization" id="toc-proximal-policy-optimization" class="nav-link" data-scroll-target="#proximal-policy-optimization">Proximal Policy Optimization</a></li>
  </ul></li>
  <li><a href="#inverse-reinforcement-learning" id="toc-inverse-reinforcement-learning" class="nav-link" data-scroll-target="#inverse-reinforcement-learning">Inverse Reinforcement Learning</a></li>
  <li><a href="#meta-reinforcement-learning" id="toc-meta-reinforcement-learning" class="nav-link" data-scroll-target="#meta-reinforcement-learning">Meta Reinforcement Learning</a></li>
  <li><a href="#reinforcement-learning-and-inference" id="toc-reinforcement-learning-and-inference" class="nav-link" data-scroll-target="#reinforcement-learning-and-inference">Reinforcement Learning and Inference</a></li>
  <li><a href="#reinforcement-learning-with-sequence-models" id="toc-reinforcement-learning-with-sequence-models" class="nav-link" data-scroll-target="#reinforcement-learning-with-sequence-models">Reinforcement Learning with Sequence Models</a></li>
  <li><a href="#reinforcement-learning-with-llm" id="toc-reinforcement-learning-with-llm" class="nav-link" data-scroll-target="#reinforcement-learning-with-llm">Reinforcement Learning with LLM</a></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference">Reference</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Overview: Deep Reinforcement Learning</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Reinforcement Learning</div>
    <div class="quarto-category">Overview</div>
  </div>
  </div>

<div>
  <div class="description">
    This is an overview of the Deep Reinforcement Learning. It includes the defintion of the deep reinforcement learning, some popular algorithms for example deep Q learning and variants, policy gradient methods and actor-critic methods for example A3C, DDPG, and PPO. Besides, it also includes the applications of those models.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">2024-10-25</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Last modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">2024-10-30</p>
    </div>
  </div>
    
  </div>
  


</header>


<p>In recent years, Deep Reinforcement Learning has emerged as one of the most promising areas within the broader field of Artificial Intelligence(AI). It combines two powerful tools: reinforcement learning(RL), where agents learn by interacting with their environment, and deep learning, where neural networks identify patterns from large amounts of data.</p>
<p>The Markov Decision Process provide the mathematical foundation for DRL, enabling agents to make intelligent decision in complex environment. Let defines some key concepts first, for more details of the MDP, please check the following <a href="https://jaz201107.github.io/posts/Basic/MDP.html">blog</a></p>
<section id="key-concepts-and-notations" class="level1">
<h1>Key Concepts and Notations</h1>
<ul>
<li><span class="math inline">\(S\)</span>: The set of all possible stats in the environment</li>
<li><span class="math inline">\(A\)</span>: The set of actions that agent can take</li>
<li><span class="math inline">\(R(s, a)\)</span>: The reward function, which gives the immediate reward received after taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(v(s)\)</span>: The State-Value function of the state, while <span class="math inline">\(V(s)\)</span> is an estimator of the true state value function <span class="math inline">\(v(s)\)</span></li>
</ul>
<p><span class="math display">\[
V^{\pi}(s) = \mathbb{E}_{\pi}\left[ \sum_{t=0}^{\infty}\gamma^{t}R_{t+1} | S_{0} = s \right]
\]</span></p>
<ul>
<li><span class="math inline">\(q(s, a)\)</span>: The State-Action function of the state-action pairs, while <span class="math inline">\(Q(s, a)\)</span> is an estimator of the true state-action function(<span class="math inline">\(q(s, a)\)</span>) <span class="math display">\[
Q^{\pi}(s, a) = \mathbb{E}_{\pi}\left[ \sum_{t=0}^{\infty}\gamma^{t}R_{t+1} | S_{0} = s, A_{0}=a \right]
\]</span> The state value function and state action value function are related through the following euqation: <span class="math display">\[
V^{\pi}(s) = \mathbb{E}_{a\sim \pi}[Q^{\pi}(s.)]
\]</span></li>
<li><span class="math inline">\(\gamma\)</span>: The discount factor <span class="math inline">\((0 \leq \gamma \leq 1)\)</span>, which determines how much the agent values future reward compared to immediate ones.</li>
<li><span class="math inline">\(G_{t}\)</span>: The return, is the cumulative reward, defined as <span class="math display">\[
G_{t} = \sum_{k=0}^{\infty}\gamma^{k}R_{t + k + 1}
\]</span></li>
<li><span class="math inline">\(A(s, a)\)</span>: The advantage function of the state-action pairs, which tells use how much better or worse a specific action <span class="math inline">\(a\)</span> is coompared to the average acction the policy would take in state <span class="math inline">\(s\)</span>.</li>
</ul>
</section>
<section id="the-objective-function" class="level1">
<h1>The Objective Function</h1>
<p>First, Let derive out objective function, the MDP can be categories according to whether the <span class="math inline">\(T\)</span> is finite and in-finite. Let discuss the finite case first: The finite MDP can be form as: <img src="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/20241030150223.png" class="img-fluid" alt="image.png"></p>
<p>The Probability Graph Model of above can be seed as: <span class="math display">\[
p_{\theta}(\underbrace{ s_{1}, a_{1}, \dots, s_{T}, a_{T} }_{ \tau }) = p(s_{1}) \prod_{t=1}^{T}\pi_{\theta}(a_{t} | s_{t})p(s_{t+1} | s_{t}, a_{t})
\]</span> The cumulative rewards of one trajectory is: <span class="math display">\[
p_{\theta}(\tau) \sum_{t}r(s_{t}, a_{t})
\]</span> And the goal of the RL is to learn a optimal policy <span class="math inline">\(\pi_{\theta^{*}}\)</span> that maximize the expected cumulative rewards along the trajectory <span class="math display">\[
\theta^{*} = \underset{\theta}{\operatorname{arg\max}\ } \mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[ \sum_{t} r(s_{t}, a_{t}) \right]
\]</span></p>
<p>On the other view, as we can see from the graph, the state and action can be paired together to get the Markov Chain: <img src="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/20241030150759.png" class="img-fluid" alt="image.png"></p>
<p>where: <span class="math display">\[
p_{\theta}((s_{t+1}, a_{t+1}) | (s_{t}, a_{t})) = p(s_{t+1} | s_{t}, a_{t})\pi_{\theta}(a_{t+1} | s_{t+1})
\]</span> The cumulative rewards of one trajectory is: <span class="math display">\[
\sum_{t}(p_{\theta}(s_{t}, a_{t})\times r(s_{t}, a_{t}))
\]</span></p>
<p>so, the optimal policy <span class="math inline">\(\pi_{\theta^{*}}\)</span> is defined as: <span class="math display">\[
\theta^{*} = \underset{\theta}{\operatorname{arg\max}\ } \sum_{t=1}^{T}\mathbb{E}_{(s_{t}, a_{t}) \sim p_{\theta}(s_{t}, a_{t})}[r(s_{t}, a_{t})]
\]</span></p>
<p>Now, let’s discuss the Infinite horizon case, according to the Markov Chain, the state action pair can be express as: <span class="math display">\[
\left(\begin{array}{}
s_{t+1} \\
a_{t+1}
\end{array} \right)
= \mathcal{T} \left(\begin{array}{}
s_{t} \\
a_{t}
\end{array} \right)
\]</span> where <span class="math inline">\(\mathcal{T}\)</span> is the state action transition operator. So, the state-action pair after <span class="math inline">\(k\)</span> steps can be express as: <span class="math display">\[
\left(\begin{array}{}
s_{t+k} \\
a_{t+k}
\end{array} \right)
= \mathcal{T}^{k} \left(\begin{array}{}
s_{t} \\
a_{t}
\end{array} \right)
\]</span> We want to check whether <span class="math inline">\(\mu = p(s_{t}, a_{t})\)</span> converge to a stationary distribution, that means: <span class="math display">\[
\mu = \mathcal{T}\mu
\]</span> As we can see, <span class="math inline">\(\mu\)</span> is the eigenvector of <span class="math inline">\(\mathcal{T}\)</span> with eigenvalue 1. It can be solved as: <span class="math display">\[
(\mathcal{T} -\mathbf{I}) \mu = 0
\]</span> :::{.callout-question} How to get <span class="math inline">\(\mathcal{T}\)</span>? :::</p>
<p>This is always exits under the some regularity conditions. And the <span class="math inline">\(\mu\)</span> is that stationary probability of this Markov Chain, so, the objective become: <span class="math display">\[
\begin{split}
\theta^{*}
&amp;= \underset{\theta}{\operatorname{arg\max}\ }  \frac{1}{T} \sum_{t}^{T} \mathbb{E}_{(s_{t}, a_{t}) \sim p_{\theta}(s_{t}, a_{t})}[r(s_{t}, a_{t})] \\
&amp;=  \mathbb{E}_{(\mathbf{s,a}) \sim p_{\theta}(\mathbf{s, a})} [r(\mathbf{s}, \mathbf{a})]
\end{split}
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In RL, we always care about the expectation, because the expectation is always smooth with respect to the parameters, which means this is the surrogate function of the objective we want. There are other surrogate function, in which re-parameter trick is the one for continuous output.</p>
</div>
</div>
<section id="algorithms" class="level2">
<h2 class="anchored" data-anchor-id="algorithms">Algorithms</h2>
<p>Most algorithm can be separated into three parts: <img src="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/20241030154749.png" class="img-fluid" alt="image.png"></p>
<ul>
<li>Generated samples: samples scan be generated from same or different policy. this is through trial-and-error</li>
<li>Fit and a model / estimate the return:
<ul>
<li>Can be fit as the evauated as the cumulative rewards</li>
<li>Can fit the model of the environment(model base): <span class="math display">\[
s_{t+1} \approx f_{\phi}(s_{t}, a_{t})
\]</span></li>
</ul></li>
<li>Improve the policy: use some optimization algorithm to optimize the parameters</li>
</ul>
<p>Samples can be expensive: * In real world * In simulator</p>
<p>The green steps can also be expensive or cheap: * Just sum over the whole reward is cheap * Try to get the model of the environment will be expensive.</p>
<p>Blue: - Just update the parameters will be cheap. - Update the parameters according to the model and policy will be expensive.</p>
<p>There are different types of algorithms: * Policy Gradient - Value-Based - Actor-Critic - Model-Based: - For Planning - For improve policy</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/20241030160220.png" class="img-fluid figure-img"></p>
<figcaption>image.png</figcaption>
</figure>
</div>
<p>Assumptions: 1. Full observability 1. Can be mitigated by adding recurrence 2. Episodic Learning 1. Often assumed by policy graident method 3. Continuity or smoothinges 1. Assumed by some continuous value function learning methods 2. Often assumed by some model-based RL methods<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</section>
</section>
<section id="q-learning-and-variants" class="level1">
<h1>Q-Learning and Variants</h1>
<section id="classical-deepq-learning" class="level2">
<h2 class="anchored" data-anchor-id="classical-deepq-learning">Classical (Deep)Q-Learning</h2>
<p>The classic Q-learning finds the optimal state-action value function <span class="math inline">\(Q(s, a)\)</span> through the Bellman Equation. The Q-value update rule is: <span class="math display">\[
Q(s_{t}, a_{t}) \leftarrow  Q(s_{t}, a_{t}) + \alpha(R_{t + 1} + \gamma \underset{a'}{\max \ }Q(s_{t+1}, a') - Q(s_{t}, a_{t}))
\]</span> When we use the neural network as function approximator, we need define an loss function to update the parameters. The Loss function is: <span class="math display">\[
\mathcal{L}(\theta) = \mathbb{E}[(R_{t+1} + \gamma \underset{a'}{\max \ }Q(s_{t+1}, a'; \theta^{-}) - Q(s_{t}, a_{t}; \theta) )^{2}]
\]</span></p>
<p>The problem of classical Q-Learning is that it suffer from overestimation bias, where the <code>max</code> operator selects actions that appear optimal due to noisy or high Q-value. To release this problem, we can use Double Q-Learning, it defined as</p>
</section>
<section id="double-deep-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="double-deep-q-learning">Double (Deep) Q-Learning</h2>
<p>Double Q-Learning reduces <strong>overestimation bias</strong> by maintaining two Q-tables(or two networks in deep learning settings). Each table is updated independently to decouple the action selection and value estimation processes. The update rule is: <span class="math display">\[
Q_{1}(s_{t}, a_{t}) \leftarrow Q_{1}(s_{t}, a_{t}) + \alpha(R_{t+1} + \gamma Q_{2}(s_{t + 1}, \underset{a'}{\operatorname{arg\max}\ }Q_{1}(s_{t+1}, a') ) - Q_{1}(s_{t}, a_{t}))
\]</span></p>
<p>The Loss Function for the Double Deep Q_Learning is that: <span class="math display">\[
\mathcal{L}(\theta)=\mathbb{E}[(R_{t+1} + \gamma Q(s_{t+1}, \underset{a'}{\operatorname{arg\max}\ }Q(s_{t+1}, a'; \theta);\theta^{-} )-Q(s_{t}, a_{t}; \theta))^{2}]
\]</span></p>
</section>
<section id="dueling-dqn" class="level2">
<h2 class="anchored" data-anchor-id="dueling-dqn">Dueling DQN</h2>
<p>The problem of the Basic DQN is that not all actions are equally relevant in every state, but DQN treats all state-action equally, to solve this problem, we can use Dueling DQN, which separates the Q-value into two components: * State-Value function <span class="math inline">\(V(s)\)</span>: measures the value of being in state <span class="math inline">\(s\)</span> * Advantage function <span class="math inline">\(A(s, a)\)</span>, which measures the relative importance of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>.</p>
<p>The Q-value is them computed as: <span class="math display">\[
Q(s, a) = V(s) + A(s, a) - \underset{a'}{\max \ } A(s, a')
\]</span></p>
</section>
<section id="noisy-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="noisy-q-learning">Noisy Q-Learning</h2>
<p>In som environments, the agents needs better exploration strategies to avoid getting suck in local optima. The Noisy-Q-Learning introduces parametric noise into the neural network’s parameters, allowing the agent to explore more efficitively</p>
</section>
<section id="soft-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="soft-q-learning">Soft Q-Learning</h2>
<p>Soft Q-learning introduces an entropy term into the objective to encourage exploration. The agent seeks to maximize both the cumulative reawrd and the policy entropy, learning yo better exploration in uncertain environemnts</p>
<p>The Objective Function become: <span class="math display">\[
J(\pi) =\mathbb{E}\left[ \sum_{t}(R_{t} + \alpha \mathcal{H}(\pi(\cdot | s_{t}))) \right]
\]</span></p>
</section>
<section id="experience-replay-buffer" class="level2">
<h2 class="anchored" data-anchor-id="experience-replay-buffer">Experience Replay Buffer</h2>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section"></h2>
</section>
</section>
<section id="policy-based-algorithm" class="level1">
<h1>Policy Based Algorithm</h1>
<p>First, let derive the object we need to optimize with in policy gradient, we define the objective as <span class="math inline">\(J(\theta)\)</span>, where <span class="math inline">\(J(\theta)\)</span> is defined as: <span class="math display">\[
\begin{split}
J(\theta)
&amp; =\mathbb{E}_{\tau \sim p_{\theta}(\tau)}[r(\tau)] \quad \text{ where }r(\tau) = \sum_{t}r(s_{t}, a_{t}) \\
&amp; = \int p_{\theta}(\tau)r(\tau) \, d\tau
\end{split}
\]</span> The gradient of the objective function is: <span class="math display">\[
\begin{split}
\nabla J(\theta)
&amp;= \int \nabla p_{\theta}(\tau)r(\tau) \, d\tau \\
&amp;=  \int p_{\theta}(\tau)\nabla \log p_{\theta}(\tau)r(\tau) \, d\tau \quad \left( \nabla \log x = \nabla x \frac{1}{\log x} \right) \\
&amp;=  \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[\nabla \log p_{\theta}(\tau)r(\tau)] \\
\end{split}
\]</span> Because the <span class="math inline">\(\log p_{\theta}(\tau)\)</span> is equal to: <span class="math display">\[
\begin{split}
\log p_{\theta}(\tau) &amp;=  \log p(s_{1}) \prod_{t=1}^{T}\pi_{\theta}(a_{t} | s_{t})p(s_{t+1} | s_{t}, a_{t}) \\
&amp; = \log p(s_{1}) + \sum_{t=1}^{T}\log \pi_{\theta}(a_{t} | s_{t}) + \log p(s_{t+1} | s_{t}, a_{t})
\end{split}
\]</span> The gradient with respect to the parameter of <span class="math inline">\(\pi\)</span> is only related to the second term, so: <span class="math display">\[
\begin{split}
\nabla J(\theta)
&amp;= \mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[ \bigg(\sum_{t=1}^{T}\nabla \log \pi_{\theta}(a_{t} | s_{t})\bigg)\left( \sum_{t}^{T}r(s_{t}, a_{t})  \right) \right]  \\
&amp; \approx  \frac{1}{N} \sum_{i=1}^{N}\bigg(\sum_{t=1}^{T}\nabla \log \pi_{\theta}(a_{i, t} | s_{i, t})\bigg)\left( \sum_{t = 1}^{T}r(s_{i,t}, a_{i,t})  \right)
\end{split}
\]</span></p>
<p>When add causaliy it can used to reduce the variance. Causality: policy at time <span class="math inline">\(t'\)</span> cannot affect reward at time <span class="math inline">\(t\)</span> when <span class="math inline">\(t &lt; t'\)</span></p>
we can get that: $$
<span class="math display">\[\begin{split}
\nabla J(\theta)

&amp; \approx  \frac{1}{N} \sum_{i=1}^{N} \left(\sum_{t=1}^{T}\nabla \log \pi_{\theta}(a_{i, t} | s_{i, t})\underbrace{ \sum_{\textcolor{red}{t' = t}}^{T}r(s_{i,\textcolor{red}{t'}}, a_{i,\textcolor{red}{t'}})}_{ G: \text{Reward to Go} }\right) \\
&amp;= \sum_{i=1}^{N} \left(\sum_{t=1}^{T}\nabla \log \pi_{\theta}(a_{i, t} | s_{i, t}) G_{t} \right)  \\
\end{split}\]</span>
<p>$$</p>
<section id="reinforce" class="level2">
<h2 class="anchored" data-anchor-id="reinforce">REINFORCE</h2>
<p>REINFORCE is the most basic algorithm used in the reinforcement learning algorithms. The basic process is: 1. sample trajectory <span class="math inline">\(\{ \tau^{i}\}\)</span> according to policy <span class="math inline">\(\pi_{\theta}\)</span> 2. Calculate the gradient of the <span class="math inline">\(J(\theta^{-})\)</span> 3. Update the parameters: <span class="math display">\[
\theta \leftarrow  \theta^{-} + \alpha \nabla_{\theta^{-}}J(\theta^{-})
\]</span></p>
<section id="reinforce-with-baseline" class="level3">
<h3 class="anchored" data-anchor-id="reinforce-with-baseline">REINFORCE with Baseline</h3>
<p>The baseline, we can make that the positive reward increase when the bad reward decrease, the basic baseline is: <span class="math display">\[
b = \frac{1}{N}\sum_{i=1}^{N}r(\tau)
\]</span> So, the objective function become: <span class="math display">\[
\nabla_{\theta}J(\theta) \approx \frac{1}{N}\sum_{i=1}^{N}\nabla _{\theta}\log p_{\theta}(\tau)[r(\tau) - b]
\]</span></p>
<p>Are we allow: <img src="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/20241030170108.png" class="img-fluid" alt="image.png"></p>
<p>Subtracting a baseline is unbiased in expectation.</p>
</section>
<section id="off-policy-reinforce" class="level3">
<h3 class="anchored" data-anchor-id="off-policy-reinforce">Off-Policy REINFORCE</h3>
<p><span class="math display">\[
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[\nabla \log p_{\theta}(\tau)r(\tau)]
\]</span> The way we calculate the objective function is by sampling trajectories using the latest policy, so need throw out the samples each time we change <span class="math inline">\(\theta\)</span>. We cannot retain samples from previous policy.</p>
<p>NN change only a little bit with each gradient step, so this might be sample expensive. So, we can transfer this into off-policy learning through important sampling.</p>
</section>
<section id="natural-gradient" class="level3">
<h3 class="anchored" data-anchor-id="natural-gradient">Natural Gradient</h3>
<p>Some parameters change probabilities a lot more than others, what we want to by changing the learning rate for different parameters, that have larger learning rate for those parameters that don’t change policy very much, in order to all the parameters to reach optimal value. Look constraints optmization view of first order graident descent,.</p>
<p>From <strong>mirror descent</strong> or <strong>projected gradient descent</strong>. <img src="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/20241030172937.png" class="img-fluid" alt="image.png"> Usually just take <span class="math inline">\(\alpha\)</span> rather then <span class="math inline">\(\epsilon\)</span> where <span class="math inline">\(\alpha\)</span> is just the lagrange multiplier that corresponds to epsilon.</p>
<p>The step size are equal size in policy space rather than rather than parameter space. which means rescale the gradients</p>
<p>Conjugate gradient works well for this.</p>
</section>
</section>
<section id="actor-critic-methods" class="level2">
<h2 class="anchored" data-anchor-id="actor-critic-methods">Actor-Critic Methods</h2>
<section id="basic-actor-critic-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="basic-actor-critic-algorithm">Basic Actor-Critic Algorithm</h3>
</section>
<section id="advantage-actor-critic-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="advantage-actor-critic-algorithm">Advantage Actor-Critic Algorithm</h3>
</section>
<section id="asyn-advantage-actor-critic-algorithm-a3c" class="level3">
<h3 class="anchored" data-anchor-id="asyn-advantage-actor-critic-algorithm-a3c">Asyn Advantage Actor-Critic Algorithm (A3C)</h3>
</section>
</section>
<section id="trust-region-policy-optimization" class="level2">
<h2 class="anchored" data-anchor-id="trust-region-policy-optimization">Trust Region Policy Optimization</h2>
<p>motivated by the natural gradient but includees additional constraints for robustness</p>
</section>
<section id="proximal-policy-optimization" class="level2">
<h2 class="anchored" data-anchor-id="proximal-policy-optimization">Proximal Policy Optimization</h2>
<p>This is an simpller alternative to TRPO which approximates the natural gradient updates through a simpler, more efficient objective function.</p>
</section>
</section>
<section id="inverse-reinforcement-learning" class="level1">
<h1>Inverse Reinforcement Learning</h1>
</section>
<section id="meta-reinforcement-learning" class="level1">
<h1>Meta Reinforcement Learning</h1>
</section>
<section id="reinforcement-learning-and-inference" class="level1">
<h1>Reinforcement Learning and Inference</h1>
</section>
<section id="reinforcement-learning-with-sequence-models" class="level1">
<h1>Reinforcement Learning with Sequence Models</h1>
</section>
<section id="reinforcement-learning-with-llm" class="level1">
<h1>Reinforcement Learning with LLM</h1>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>
<p>[1] <a href="https://spinningup.openai.com/en/latest/">OpenAI Spinning Up</a> [2] <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">Lil’ Log Policy Gradient Algorithms</a> [3] <a href="https://rail.eecs.berkeley.edu/deeprlcourse/">UCB CS 285 Deep Reinforcement Learning 2023</a> [4] <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Andrew Barto and Richard S. Sutto: Reinforcement Learning an Introducntion</a></p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>1<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>