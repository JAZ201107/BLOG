<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-10-25">
<meta name="description" content="This is an overview of the Deep Reinforcement Learning. It includes the defintion of the deep reinforcement learning, some popular algorithms for example deep Q learning and variants, policy gradient methods and actor-critic methods for example A3C, DDPG, and PPO. Besides, it also includes the applications of those models.">

<title>Overview: Deep Reinforcement Learning – Yuyang’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Overview: Deep Reinforcement Learning – Yuyang’s Blog">
<meta property="og:description" content="This is an overview of the Deep Reinforcement Learning. It includes the defintion of the deep reinforcement learning, some popular algorithms for example deep Q learning and variants, policy gradient methods and actor-critic methods for example A3C, DDPG, and PPO. Besides, it also includes the applications of those models.">
<meta property="og:image" content="Images/Overview_DRL.png">
<meta property="og:site_name" content="Yuyang's Blog">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Yuyang’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/JAZ201107"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/zhang-yuyang/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="1">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#key-concepts-and-notations" id="toc-key-concepts-and-notations" class="nav-link active" data-scroll-target="#key-concepts-and-notations">Key Concepts and Notations</a></li>
  <li><a href="#the-objective-function" id="toc-the-objective-function" class="nav-link" data-scroll-target="#the-objective-function">The Objective Function</a>
  <ul class="collapse">
  <li><a href="#algorithms" id="toc-algorithms" class="nav-link" data-scroll-target="#algorithms">Algorithms</a></li>
  </ul></li>
  <li><a href="#q-learning-and-variants" id="toc-q-learning-and-variants" class="nav-link" data-scroll-target="#q-learning-and-variants">Q-Learning and Variants</a>
  <ul class="collapse">
  <li><a href="#classical-deepq-learning" id="toc-classical-deepq-learning" class="nav-link" data-scroll-target="#classical-deepq-learning">Classical (Deep)Q-Learning</a></li>
  <li><a href="#double-deep-q-learning" id="toc-double-deep-q-learning" class="nav-link" data-scroll-target="#double-deep-q-learning">Double (Deep) Q-Learning</a></li>
  <li><a href="#dueling-dqn" id="toc-dueling-dqn" class="nav-link" data-scroll-target="#dueling-dqn">Dueling DQN</a></li>
  <li><a href="#noisy-q-learning" id="toc-noisy-q-learning" class="nav-link" data-scroll-target="#noisy-q-learning">Noisy Q-Learning</a></li>
  <li><a href="#soft-q-learning" id="toc-soft-q-learning" class="nav-link" data-scroll-target="#soft-q-learning">Soft Q-Learning</a></li>
  <li><a href="#experience-replay-buffer" id="toc-experience-replay-buffer" class="nav-link" data-scroll-target="#experience-replay-buffer">Experience Replay Buffer</a>
  <ul class="collapse">
  <li><a href="#basic-experienc-replay-buffer" id="toc-basic-experienc-replay-buffer" class="nav-link" data-scroll-target="#basic-experienc-replay-buffer">Basic Experienc Replay Buffer</a></li>
  <li><a href="#prioritized-experience-replayper" id="toc-prioritized-experience-replayper" class="nav-link" data-scroll-target="#prioritized-experience-replayper">Prioritized Experience Replay(PER)</a></li>
  </ul></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"></a>
  <ul class="collapse">
  <li><a href="#hindsight-experience-replay-her" id="toc-hindsight-experience-replay-her" class="nav-link" data-scroll-target="#hindsight-experience-replay-her">Hindsight Experience Replay (HER)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#policy-based-algorithm" id="toc-policy-based-algorithm" class="nav-link" data-scroll-target="#policy-based-algorithm">Policy Based Algorithm</a>
  <ul class="collapse">
  <li><a href="#reinforce" id="toc-reinforce" class="nav-link" data-scroll-target="#reinforce">REINFORCE</a>
  <ul class="collapse">
  <li><a href="#reinforce-with-baseline" id="toc-reinforce-with-baseline" class="nav-link" data-scroll-target="#reinforce-with-baseline">REINFORCE with Baseline</a></li>
  <li><a href="#off-policy-reinforce" id="toc-off-policy-reinforce" class="nav-link" data-scroll-target="#off-policy-reinforce">Off-Policy REINFORCE</a></li>
  <li><a href="#natural-gradient" id="toc-natural-gradient" class="nav-link" data-scroll-target="#natural-gradient">Natural Gradient</a></li>
  </ul></li>
  <li><a href="#actor-critic-methods" id="toc-actor-critic-methods" class="nav-link" data-scroll-target="#actor-critic-methods">Actor-Critic Methods</a>
  <ul class="collapse">
  <li><a href="#basic-actor-critic-algorithm" id="toc-basic-actor-critic-algorithm" class="nav-link" data-scroll-target="#basic-actor-critic-algorithm">Basic Actor-Critic Algorithm</a></li>
  <li><a href="#advantage-actor-critic-algorithm" id="toc-advantage-actor-critic-algorithm" class="nav-link" data-scroll-target="#advantage-actor-critic-algorithm">Advantage Actor-Critic Algorithm</a></li>
  <li><a href="#asyn-advantage-actor-critic-algorithm-a3c" id="toc-asyn-advantage-actor-critic-algorithm-a3c" class="nav-link" data-scroll-target="#asyn-advantage-actor-critic-algorithm-a3c">Asyn Advantage Actor-Critic Algorithm (A3C)</a></li>
  </ul></li>
  <li><a href="#trust-region-policy-optimization" id="toc-trust-region-policy-optimization" class="nav-link" data-scroll-target="#trust-region-policy-optimization">Trust Region Policy Optimization</a></li>
  <li><a href="#proximal-policy-optimization" id="toc-proximal-policy-optimization" class="nav-link" data-scroll-target="#proximal-policy-optimization">Proximal Policy Optimization</a></li>
  </ul></li>
  <li><a href="#inverse-reinforcement-learning" id="toc-inverse-reinforcement-learning" class="nav-link" data-scroll-target="#inverse-reinforcement-learning">Inverse Reinforcement Learning</a></li>
  <li><a href="#meta-reinforcement-learning" id="toc-meta-reinforcement-learning" class="nav-link" data-scroll-target="#meta-reinforcement-learning">Meta Reinforcement Learning</a></li>
  <li><a href="#reinforcement-learning-and-inference" id="toc-reinforcement-learning-and-inference" class="nav-link" data-scroll-target="#reinforcement-learning-and-inference">Reinforcement Learning and Inference</a></li>
  <li><a href="#reinforcement-learning-with-sequence-models" id="toc-reinforcement-learning-with-sequence-models" class="nav-link" data-scroll-target="#reinforcement-learning-with-sequence-models">Reinforcement Learning with Sequence Models</a></li>
  <li><a href="#reinforcement-learning-with-llm" id="toc-reinforcement-learning-with-llm" class="nav-link" data-scroll-target="#reinforcement-learning-with-llm">Reinforcement Learning with LLM</a></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference">Reference</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Overview: Deep Reinforcement Learning</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Reinforcement Learning</div>
    <div class="quarto-category">Overview</div>
  </div>
  </div>

<div>
  <div class="description">
    This is an overview of the Deep Reinforcement Learning. It includes the defintion of the deep reinforcement learning, some popular algorithms for example deep Q learning and variants, policy gradient methods and actor-critic methods for example A3C, DDPG, and PPO. Besides, it also includes the applications of those models.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">2024-10-25</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Last modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">2024-10-31</p>
    </div>
  </div>
    
  </div>
  


</header>


<p>In recent years, Deep Reinforcement Learning has emerged as one of the most promising areas within the broader field of Artificial Intelligence(AI). It combines two powerful tools: reinforcement learning(RL), where agents learn by interacting with their environment, and deep learning, where neural networks identify patterns from large amounts of data.</p>
<p>The <strong>Markov Decision Process</strong> provide the mathematical foundation for DRL, enabling agents to make intelligent decision in complex environment. Let defines some key concepts first, but for more details of the MDP, please check this <a href="https://jaz201107.github.io/posts/Basic/MDP.html">blog</a></p>
<section id="key-concepts-and-notations" class="level1">
<h1>Key Concepts and Notations</h1>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span>: The set of all possible stats in the environment</li>
<li><span class="math inline">\(\mathcal{A}\)</span>: The set of actions that agent can take</li>
<li><span class="math inline">\(r(s, a)\)</span>: The <strong>reward function</strong>, which gives the immediate reward received after taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, while <span class="math inline">\(R_t\)</span> is the reward at time <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(v(s)\)</span>: The State-Value function of the state, while <span class="math inline">\(V(s)\)</span> is an <em>estimator</em> of the true state value function <span class="math inline">\(v(s)\)</span></li>
</ul>
<p><span id="eq-valuefunction"><span class="math display">\[
V^{\pi}(s) = \mathbb{E}_{\pi}\left[ \sum_{t=0}^{\infty}\gamma^{t}R_{t+1} | S_{0} = s \right]
\tag{1}\]</span></span></p>
<ul>
<li><p><span class="math inline">\(q(s, a)\)</span>: The State-Action function of the state-action pairs, while <span class="math inline">\(Q(s, a)\)</span> is an <em>estimator</em> of the true state-action function<span class="math inline">\(q(s, a)\)</span> <span id="eq-qfunction"><span class="math display">\[
Q^{\pi}(s, a) = \mathbb{E}_{\pi}\left[ \sum_{t=0}^{\infty}\gamma^{t}R_{t+1} | S_{0} = s, A_{0}=a \right]
\tag{2}\]</span></span> The state value function and state action value function are related through the following equation: <span class="math display">\[
V^{\pi}(s) = \mathbb{E}_{a\sim \pi}[Q^{\pi}(s)]
\]</span></p></li>
<li><p><span class="math inline">\(\gamma\)</span>: The <strong>discount factor</strong> <span class="math inline">\((0 \leq \gamma \leq 1)\)</span>, which determines how much the agent values future reward compared to immediate ones. It is also a way to deal with infinite situation.</p></li>
<li><p><span class="math inline">\(G_{t}\)</span>: The return, is the <strong>cumulative reward</strong>, defined as <span id="eq-cumulate_return"><span class="math display">\[
G_{t} = \sum_{k=0}^{\infty}\gamma^{k}R_{t + k + 1}
\tag{3}\]</span></span></p></li>
<li><p><span class="math inline">\(A(s, a)\)</span>: The <strong>advantage function</strong> of the state-action pairs, which tells use how much better or worse a specific action <span class="math inline">\(a\)</span> is compared to the average action the policy would take in state <span class="math inline">\(s\)</span>, defined as</p></li>
</ul>
<p><span id="eq-adv-function"><span class="math display">\[
A(s, a) = Q(s, a) - V(s)
\tag{4}\]</span></span></p>
</section>
<section id="the-objective-function" class="level1">
<h1>The Objective Function</h1>
<p>First, Let derive out <strong>objective function</strong>. the MDP can be categorized according to whether the <span class="math inline">\(T\)</span> is <em>finite</em> and <em>in-finite</em>.</p>
<p>Let discuss the finite case firs. The finite MDP can be form as: <img src="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/20241030150223.png" class="img-fluid" alt="Markov Decision Process for RL"></p>
<p>The Probability Graphic Model of above can be seed as <span class="math display">\[
p_{\theta}(\underbrace{ s_{1}, a_{1}, \dots, s_{T}, a_{T} }_{ \tau }) = p(s_{1}) \prod_{t=1}^{T}\pi_{\theta}(a_{t} | s_{t})p(s_{t+1} | s_{t}, a_{t})
\]</span> The <strong>cumulative rewards</strong> of one trajectory is: <span class="math display">\[
p_{\theta}(\tau) \sum_{t}r(s_{t}, a_{t})
\]</span> And the goal of the RL is to learn a optimal policy <span class="math inline">\(\pi_{\theta^{*}}\)</span> that <u>maximize the expected cumulative rewards along the trajectory</u> <span class="math display">\[
\theta^{*} = \underset{\theta}{\operatorname{arg\max}\ } \mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[ \sum_{t} r(s_{t}, a_{t}) \right]
\]</span></p>
<p>On the other view, as we can see from the graph, the state and action can be paired together to get the <strong>Markov Chain</strong>: <img src="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/20241030150759.png" class="img-fluid" alt="image.png"></p>
<p>where: <span class="math display">\[
p_{\theta}((s_{t+1}, a_{t+1}) | (s_{t}, a_{t})) = p(s_{t+1} | s_{t}, a_{t})\pi_{\theta}(a_{t+1} | s_{t+1})
\]</span> The cumulative rewards of one trajectory is: <span class="math display">\[
\sum_{t}p_{\theta}(s_{t}, a_{t})\times r(s_{t}, a_{t})
\]</span></p>
<p>so, the optimal policy <span class="math inline">\(\pi_{\theta^{*}}\)</span> is defined as: <span class="math display">\[
\theta^{*} = \underset{\theta}{\operatorname{arg\max}\ } \sum_{t=1}^{T}\mathbb{E}_{(s_{t}, a_{t}) \sim p_{\theta}(s_{t}, a_{t})}[r(s_{t}, a_{t})]
\]</span></p>
<p>Now, let’s discuss the <strong>Infinite horizon case</strong>, according to the Markov Chain, the state action pair can be express as: <span class="math display">\[
\left(\begin{array}{}
s_{t+1} \\
a_{t+1}
\end{array} \right)
= \mathcal{T} \left(\begin{array}{}
s_{t} \\
a_{t}
\end{array} \right)
\]</span> where <span class="math inline">\(\mathcal{T}\)</span> is the <strong>state action transition operator</strong>. So, the state-action pair after <span class="math inline">\(k\)</span> steps can be express as: <span class="math display">\[
\left(\begin{array}{}
s_{t+k} \\
a_{t+k}
\end{array} \right)
= \mathcal{T}^{k} \left(\begin{array}{}
s_{t} \\
a_{t}
\end{array} \right)
\]</span> We want to check whether <span class="math inline">\(\mu = p(s_{t}, a_{t})\)</span> converge to a stationary distribution, that means: <span class="math display">\[
\mu = \mathcal{T}\mu
\]</span> As we can see, <span class="math inline">\(\mu\)</span> is the eigenvector of <span class="math inline">\(\mathcal{T}\)</span> with eigenvalue 1. It can be solved as: <span class="math display">\[
(\mathcal{T} -\mathbf{I}) \mu = 0
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How to get $\mathcal{T}$
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p>This is always exits under the some regularity conditions. And the <span class="math inline">\(\mu\)</span> is that <strong>stationary probability</strong> of this Markov Chain, so, the objective become: <span class="math display">\[
\begin{split}
\theta^{*}
&amp;= \underset{\theta}{\operatorname{arg\max}\ }  \frac{1}{T} \sum_{t}^{T} \mathbb{E}_{(s_{t}, a_{t}) \sim p_{\theta}(s_{t}, a_{t})}[r(s_{t}, a_{t})] \\
&amp;= \underset{\theta}{\operatorname{arg\max}\ } \mathbb{E}_{(\mathbf{s,a}) \sim p_{\theta}(\mathbf{s, a})} [r(\mathbf{s}, \mathbf{a})]
\end{split}
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In RL, we always care about the expectation, because the expectation is always smooth with respect to the parameters, which means this is the surrogate function of the objective we want. There are other surrogate function, in which re-parameter trick is the one for continuous output.</p>
</div>
</div>
<section id="algorithms" class="level2">
<h2 class="anchored" data-anchor-id="algorithms">Algorithms</h2>
<p>Most algorithm can be separated into three parts: <img src="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/20241030154749.png" class="img-fluid" width="728" alt="RL Algorithms Anatomy"></p>
<ul>
<li><strong>Generated samples</strong>: samples scan be generated from <em>same</em> or <em>different</em> policy, this is through trial-and-error</li>
<li><strong>Fit and a model / estimate the return</strong>:
<ul>
<li>Can be fit as the evauated as the cumulative rewards(model free)</li>
<li>Can fit the model of the environment(model base): <span class="math display">\[
s_{t+1} \approx f_{\phi}(s_{t}, a_{t})
\]</span></li>
</ul></li>
<li><strong>Improve the policy</strong>: use some optimization algorithms(e.g.&nbsp;<strong>gradient ascent</strong>) to optimize the parameters</li>
</ul>
<p>The orange can be expensive:</p>
<ul>
<li>In real world: this is expensive because we can not speed up the sampling process</li>
<li>In simulator: this is much cheap.</li>
</ul>
<p>The green steps can also be expensive or cheap:</p>
<ul>
<li>Just sum over the whole reward is cheap</li>
<li>Try to get the model of the environment will be expensive.</li>
</ul>
<p>The Blue also has different cost:</p>
<ul>
<li>Just update the parameters will be cheap.</li>
<li>Update the parameters according to the model and policy will be expensive.</li>
</ul>
<p>There are different types of algorithms:</p>
<ul>
<li>Policy Gradient</li>
<li>Value-Based</li>
<li>Actor-Critic</li>
<li>Model-Based:
<ul>
<li>For Planning</li>
<li>For improve policy</li>
</ul></li>
</ul>
<p>Now, let’s introduce some specific algorithms, we first start with Deep Q Network, which was first introduced by <span class="citation" data-cites="playingatarideepreinforcementlearning2013">Mnih et al. (<a href="#ref-playingatarideepreinforcementlearning2013" role="doc-biblioref">2013</a>)</span></p>
</section>
</section>
<section id="q-learning-and-variants" class="level1">
<h1>Q-Learning and Variants</h1>
<section id="classical-deepq-learning" class="level2">
<h2 class="anchored" data-anchor-id="classical-deepq-learning">Classical (Deep)Q-Learning</h2>
<p>The classic Q-learning finds the optimal state-action value function <span class="math inline">\(Q^*(s, a)\)</span> through the Bellman Equation. The Q-value update rule is: <span class="math display">\[
Q(s_{t}, a_{t}) \leftarrow  Q(s_{t}, a_{t}) + \alpha(R_{t + 1} + \gamma \underset{a'}{\max \ }Q(s_{t+1}, a') - Q(s_{t}, a_{t}))
\]</span></p>
<p>When we use the neural network as <strong>function approximator</strong>, we need define an <strong>loss function</strong> to update the parameters. The Loss function is: <span class="math display">\[
\mathcal{L}(\theta) = \mathbb{E}[(R_{t+1} + \gamma \underset{a'}{\max \ }Q(s_{t+1}, a'; \theta^{-}) - Q(s_{t}, a_{t}; \theta) )^{2}]
\]</span></p>
<p>The problem of classical Q-Learning is that it suffer from <strong>overestimation bias</strong>, where the <code>max</code> operator selects actions that appear optimal due to noisy or high Q-value. To release this problem, we can use Double Q-Learning, it defined as</p>
</section>
<section id="double-deep-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="double-deep-q-learning">Double (Deep) Q-Learning</h2>
<p>Double Q-Learning<span class="citation" data-cites="deepreinforcementlearningdoubleqlearning2015">(<a href="#ref-deepreinforcementlearningdoubleqlearning2015" role="doc-biblioref">Hasselt, Guez, and Silver 2015</a>)</span> reduces <strong>overestimation bias</strong> by maintaining two Q-tables(or two networks in deep learning settings). Each table is updated independently to decouple the action selection and value estimation processes. The update rule is: <span class="math display">\[
Q_{1}(s_{t}, a_{t}) \leftarrow Q_{1}(s_{t}, a_{t}) + \alpha(R_{t+1} + \gamma Q_{2}(s_{t + 1}, \underset{a'}{\operatorname{arg\max}\ }Q_{1}(s_{t+1}, a') ) - Q_{1}(s_{t}, a_{t}))
\]</span></p>
<p>The Loss Function for the Double Deep Q_Learning is that: <span class="math display">\[
\mathcal{L}(\theta)=\mathbb{E}[(R_{t+1} + \gamma Q(s_{t+1}, \underset{a'}{\operatorname{arg\max}\ }Q(s_{t+1}, a'; \theta);\theta^{-} )-Q(s_{t}, a_{t}; \theta))^{2}]
\]</span></p>
</section>
<section id="dueling-dqn" class="level2">
<h2 class="anchored" data-anchor-id="dueling-dqn">Dueling DQN</h2>
<p>The problem of the Basic DQN is that not all actions are equally relevant in every state, but DQN treats all state-action equally, to solve this problem, we can use Dueling DQN <span class="citation" data-cites="duelingnetworkarchitecturesdeepreinforcementlearning2016">(<a href="#ref-duelingnetworkarchitecturesdeepreinforcementlearning2016" role="doc-biblioref">Wang et al. 2016</a>)</span>, which separates the Q-value into two components:</p>
<ul>
<li><strong>State-Value function</strong> <span class="math inline">\(V(s)\)</span>: measures the value of being in state <span class="math inline">\(s\)</span></li>
<li><strong>Advantage function</strong> <span class="math inline">\(A(s, a)\)</span>, which measures the <em>relative importance</em> of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>.</li>
</ul>
<p>The Q-value is them computed as: <span class="math display">\[
Q(s, a) = V(s) + A(s, a) - \underset{a'}{\max \ } A(s, a')
\]</span></p>
</section>
<section id="noisy-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="noisy-q-learning">Noisy Q-Learning</h2>
<p>In some environments, the agents needs better exploration strategies to avoid getting suck in local optima. The Noisy-Q-Network <span class="citation" data-cites="noisynetworksexploration2019">(<a href="#ref-noisynetworksexploration2019" role="doc-biblioref">Fortunato et al. 2019</a>)</span> introduces <strong>parametric noise</strong> into the neural network’s parameters, allowing the agent to explore more effectively</p>
</section>
<section id="soft-q-learning" class="level2">
<h2 class="anchored" data-anchor-id="soft-q-learning">Soft Q-Learning</h2>
<p>Soft Q-learning<span class="citation" data-cites="haarnojaReinforcementLearningDeep2017">(<a href="#ref-haarnojaReinforcementLearningDeep2017" role="doc-biblioref">Haarnoja et al. 2017</a>)</span> introduces an <strong>entropy</strong> term into the objective to encourage exploration. The agent seeks to maximize both <u>the cumulative reward</u> and the <u>policy entropy</u>, learning yo better exploration in uncertain environments</p>
<p>The Objective Function become: <span class="math display">\[
J(\pi) =\mathbb{E}\left[ \sum_{t}(R_{t} + \alpha \mathcal{H}(\pi(\cdot | s_{t}))) \right]
\]</span></p>
</section>
<section id="experience-replay-buffer" class="level2">
<h2 class="anchored" data-anchor-id="experience-replay-buffer">Experience Replay Buffer</h2>
<p>One of the vital component in the RL toolkit that boosts learning and stable is the training is the Experience Replay Buffer. There are several reason to use the Experience Replay Buffer:</p>
<ul>
<li>Improved Sample Efficiency: Collecting new experience constantly is costly and inefficient. By storing past experiences, the agent can use each iteration multiple times</li>
<li>Breaking Temporal Correlations: Learning sequentially from consecutive experiences introduces high correlations, which destabilize the neural network’s updates.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li>Stabilizing Learning: Replay buffers ensure that the training data is diverse, including experiences from various states and actions. This helps prevent overfitting to recent experiences and ensures that rare but valuable experiences (like achieving a goal or encountering a penalty) are reused and learned from.</li>
<li>Handling Sparse Rewards: In some environments, rewards are sparse or rare.</li>
</ul>
<section id="basic-experienc-replay-buffer" class="level3">
<h3 class="anchored" data-anchor-id="basic-experienc-replay-buffer">Basic Experienc Replay Buffer</h3>
<p>The basic replay buffer is a list contain a tuple of <span class="math inline">\((s, a, r, s', d)\)</span>, which captures one interaction in the environment.</p>
</section>
<section id="prioritized-experience-replayper" class="level3">
<h3 class="anchored" data-anchor-id="prioritized-experience-replayper">Prioritized Experience Replay(PER)</h3>
<p><strong>Prioritized Experience Replay</strong> enhances the standard buffer by focusing on experiences that are more informative. Experiences that yield higher learning signals (or TD errors) are prioritized because they represent areas where the agent has the most to learn.</p>
</section>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section"></h2>
<section id="hindsight-experience-replay-her" class="level3">
<h3 class="anchored" data-anchor-id="hindsight-experience-replay-her">Hindsight Experience Replay (HER)</h3>
<p><strong>Hindsight Experience Replay</strong> is designed for environments where rewards are sparse, such as goal-directed tasks. In HER, the agent reinterprets experiences as if the achieved outcome was intended, enabling it to learn even from unsuccessful episodes by using alternate goals.</p>
</section>
</section>
</section>
<section id="policy-based-algorithm" class="level1">
<h1>Policy Based Algorithm</h1>
<p>First, let derive the object we need to optimize with in policy gradient, we define the objective as <span class="math inline">\(J(\theta)\)</span>, where <span class="math inline">\(J(\theta)\)</span> is defined as: <span class="math display">\[
\begin{split}
J(\theta)
&amp; =\mathbb{E}_{\tau \sim p_{\theta}(\tau)}[r(\tau)] \quad \text{ where }r(\tau) = \sum_{t=1}\gamma^{t-1} r(s_{t}, a_{t}) \\
&amp; = \int p_{\theta}(\tau)r(\tau) \, d\tau
\end{split}
\]</span> The <strong>gradient</strong> of the objective function is: <span class="math display">\[
\begin{split}
\nabla J(\theta)
&amp;= \int \nabla p_{\theta}(\tau)r(\tau) \, d\tau \\
&amp;=  \int p_{\theta}(\tau)\nabla \log p_{\theta}(\tau)r(\tau) \, d\tau \quad \left( \nabla \log x = \nabla x \frac{1}{x} \implies \nabla x  = x \nabla \log x \right) \\
&amp;=  \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[\nabla \log p_{\theta}(\tau)r(\tau)] \\
\end{split}
\]</span> Because the <span class="math inline">\(\log p_{\theta}(\tau)\)</span> is equal to: <span class="math display">\[
\begin{split}
\log p_{\theta}(\tau) &amp;=  \log p(s_{1}) \prod_{t=1}^{T}\pi_{\theta}(a_{t} | s_{t})p(s_{t+1} | s_{t}, a_{t}) \\
&amp; = \log p(s_{1}) + \sum_{t=1}^{T}\log \pi_{\theta}(a_{t} | s_{t}) + \log p(s_{t+1} | s_{t}, a_{t})
\end{split}
\]</span> The gradient with respect to the parameter of <span class="math inline">\(\pi\)</span> is only related to the <em>second term</em>, so: <span class="math display">\[
\begin{split}
\nabla J(\theta)
&amp;= \mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[ \bigg(\sum_{t=1}^{T}\nabla \log \pi_{\theta}(a_{t} | s_{t})\bigg)\left( \sum_{t=1}^{T}\gamma^{t-1} r(s_{t}, a_{t})  \right) \right]  \\
&amp; \approx  \frac{1}{N} \sum_{i=1}^{N}\bigg(\sum_{t=1}^{T}\nabla \log \pi_{\theta}(a_{i, t} | s_{i, t})\bigg)\left( \sum_{t = 1}^{T}\gamma^{t-1} r(s_{i,t}, a_{i,t})  \right)
\end{split}
\]</span></p>
<p>When add <strong>causality</strong><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> it can used to reduce the variance, we can get that:</p>
<p><span class="math display">\[
\begin{split}\nabla J(\theta) &amp; \approx  \frac{1}{N} \sum_{i=1}^{N} \left(\sum_{t=1}^{T}\nabla \log \pi_{\theta}(a_{i, t} | s_{i, t})\underbrace{ \sum_{\textcolor{red}{t' = t}}^{T} \gamma^{t'-t}r(s_{i,\textcolor{red}{t'}}, a_{i,\textcolor{red}{t'}})}_{ G: \text{Reward to Go} }\right) \\
&amp;= \sum_{i=1}^{N} \left(\sum_{t=1}^{T}\nabla \log \pi_{\theta}(a_{i, t} | s_{i, t}) G_{t} \right)  \\\end{split}
\]</span></p>
<section id="reinforce" class="level2">
<h2 class="anchored" data-anchor-id="reinforce">REINFORCE</h2>
<p>REINFORCE is the most basic algorithm used in the reinforcement learning algorithms. The basic process is:</p>
<ol type="1">
<li>Sample trajectory <span class="math inline">\(\{ \tau^{i}\}\)</span> according to policy <span class="math inline">\(\pi_{\theta}\)</span></li>
<li>Calculate the gradient of the <span class="math inline">\(J(\theta^{-})\)</span></li>
<li>Update the parameters: <span class="math display">\[
\theta \leftarrow  \theta^{-} + \alpha \nabla_{\theta^{-}}J(\theta^{-})
\]</span></li>
</ol>
<section id="reinforce-with-baseline" class="level3">
<h3 class="anchored" data-anchor-id="reinforce-with-baseline">REINFORCE with Baseline</h3>
<p>The baseline, we can make that the positive reward increase when the bad reward decrease, the basic baseline is: <span class="math display">\[
b = \frac{1}{N}\sum_{i=1}^{N}r(\tau)
\]</span> So, the objective function become: <span class="math display">\[
\nabla_{\theta}J(\theta) \approx \frac{1}{N}\sum_{i=1}^{N}\nabla _{\theta}\log p_{\theta}(\tau)[r(\tau) - b]
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why can we add baseline
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can adding a baseline <span class="math inline">\(b\)</span> to the gradient because it does not affect the expected value of the policy gradient:</p>
<p><span class="math display">\[
\mathbb{E}[\nabla_\theta \log p_\theta (\tau) b] = \int p_\theta (\tau) \nabla_\theta \log p_\theta (\tau) b \, d\tau = \int \nabla_\theta p_\theta (\tau) b \, d\tau = b \nabla_\theta \int p_\theta (\tau) \, d\tau = b \nabla_\theta 1 = 0
\]</span></p>
<p>Subtracting a baseline is unbiased in expectation.</p>
</div>
</div>
<p>A common choice for the baseline is the value function <span class="math inline">\(V(s)\)</span>.</p>
</section>
<section id="off-policy-reinforce" class="level3">
<h3 class="anchored" data-anchor-id="off-policy-reinforce">Off-Policy REINFORCE</h3>
<p><span class="math display">\[
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim p_{\theta}(\tau)}[\nabla \log p_{\theta}(\tau)r(\tau)]
\]</span> The way we calculate the objective function is by sampling trajectories using the latest policy, so need throw out the samples each time we change <span class="math inline">\(\theta\)</span>. We cannot retain samples from previous policy.</p>
<p>However neural network change only a little bit with each gradient step, so this might be sample expensive. So, we can transfer this into off-policy learning through <strong>important sampling</strong>.</p>
</section>
<section id="natural-gradient" class="level3">
<h3 class="anchored" data-anchor-id="natural-gradient">Natural Gradient</h3>
<p>Some parameters change probabilities a lot more than others, what we want to by changing the learning rate for different parameters, that have larger learning rate for those parameters that don’t change policy very much, in order to all the parameters to reach optimal value. Look constraints optimization view of first order graident descent,.</p>
<p>From <strong>mirror descent</strong> or <strong>projected gradient descent</strong>. <img src="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/20241030172937.png" class="img-fluid" alt="image.png"> Usually just take <span class="math inline">\(\alpha\)</span> rather then <span class="math inline">\(\epsilon\)</span> where <span class="math inline">\(\alpha\)</span> is just the lagrange multiplier that corresponds to epsilon.</p>
<p>The step size are equal size in policy space rather than rather than parameter space. which means rescale the gradients</p>
<p>Conjugate gradient works well for this.</p>
</section>
</section>
<section id="actor-critic-methods" class="level2">
<h2 class="anchored" data-anchor-id="actor-critic-methods">Actor-Critic Methods</h2>
<section id="basic-actor-critic-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="basic-actor-critic-algorithm">Basic Actor-Critic Algorithm</h3>
</section>
<section id="advantage-actor-critic-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="advantage-actor-critic-algorithm">Advantage Actor-Critic Algorithm</h3>
</section>
<section id="asyn-advantage-actor-critic-algorithm-a3c" class="level3">
<h3 class="anchored" data-anchor-id="asyn-advantage-actor-critic-algorithm-a3c">Asyn Advantage Actor-Critic Algorithm (A3C)</h3>
</section>
</section>
<section id="trust-region-policy-optimization" class="level2">
<h2 class="anchored" data-anchor-id="trust-region-policy-optimization">Trust Region Policy Optimization</h2>
<p>motivated by the natural gradient but includes additional constraints for robustness</p>
</section>
<section id="proximal-policy-optimization" class="level2">
<h2 class="anchored" data-anchor-id="proximal-policy-optimization">Proximal Policy Optimization</h2>
<p>This is an simpler alternative to TRPO which approximates the natural gradient updates through a simpler, more efficient objective function.</p>
</section>
</section>
<section id="inverse-reinforcement-learning" class="level1">
<h1>Inverse Reinforcement Learning</h1>
</section>
<section id="meta-reinforcement-learning" class="level1">
<h1>Meta Reinforcement Learning</h1>
</section>
<section id="reinforcement-learning-and-inference" class="level1">
<h1>Reinforcement Learning and Inference</h1>
</section>
<section id="reinforcement-learning-with-sequence-models" class="level1">
<h1>Reinforcement Learning with Sequence Models</h1>
</section>
<section id="reinforcement-learning-with-llm" class="level1">
<h1>Reinforcement Learning with LLM</h1>
</section>
<section id="reference" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">Reference</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-noisynetworksexploration2019" class="csl-entry" role="listitem">
Fortunato, Meire, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, et al. 2019. <span>“Noisy <span>Networks</span> for <span>Exploration</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1706.10295">https://doi.org/10.48550/arXiv.1706.10295</a>.
</div>
<div id="ref-haarnojaReinforcementLearningDeep2017" class="csl-entry" role="listitem">
Haarnoja, Tuomas, Haoran Tang, Pieter Abbeel, and Sergey Levine. 2017. <span>“Reinforcement <span>Learning</span> with <span>Deep Energy-Based Policies</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1702.08165">https://doi.org/10.48550/arXiv.1702.08165</a>.
</div>
<div id="ref-deepreinforcementlearningdoubleqlearning2015" class="csl-entry" role="listitem">
Hasselt, Hado van, Arthur Guez, and David Silver. 2015. <span>“Deep <span>Reinforcement Learning</span> with <span class="nocase">Double Q-learning</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1509.06461">https://doi.org/10.48550/arXiv.1509.06461</a>.
</div>
<div id="ref-playingatarideepreinforcementlearning2013" class="csl-entry" role="listitem">
Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. <span>“Playing <span>Atari</span> with <span>Deep Reinforcement Learning</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1312.5602">https://doi.org/10.48550/arXiv.1312.5602</a>.
</div>
<div id="ref-duelingnetworkarchitecturesdeepreinforcementlearning2016" class="csl-entry" role="listitem">
Wang, Ziyu, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. 2016. <span>“Dueling <span>Network Architectures</span> for <span>Deep Reinforcement Learning</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1511.06581">https://doi.org/10.48550/arXiv.1511.06581</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This is because the model may “overfitting” on recent patterns or states, making it difficult to generalize across states<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><strong>Causality</strong>: policy at time <span class="math inline">\(t'\)</span> cannot affect reward at time <span class="math inline">\(t\)</span> when <span class="math inline">\(t &lt; t'\)</span><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>